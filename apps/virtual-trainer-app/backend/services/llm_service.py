"""
LLM ÑÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Virtual Trainer
Ð˜Ð½Ñ‚ÐµÐ³Ñ€Ð°Ñ†Ð¸Ñ Ñ OpenAI API Ð¸ Ð¾Ð±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð²ÑÐµÑ… Ñ‚Ð¸Ð¿Ð¾Ð² LLM Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
"""

import json
import time
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
import asyncio
from loguru import logger
from openai import AsyncOpenAI

from backend.core.config import settings
from backend.core.exceptions import LLMServiceError, ValidationError
from backend.services.rag_tools_service import rag_tools
from backend.services.knowledge_base_service import knowledge_base
from backend.services.llm_service_rag_methods import LLMServiceRAGMethods
from backend.database.models import LLMRequestType


class LLMService:
    """Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ñ€Ð°Ð±Ð¾Ñ‚Ñ‹ Ñ ÑÐ·Ñ‹ÐºÐ¾Ð²Ñ‹Ð¼Ð¸ Ð¼Ð¾Ð´ÐµÐ»ÑÐ¼Ð¸"""
    
    def __init__(self):
        # ÐÐ°ÑÑ‚Ñ€Ð¾Ð¹ÐºÐ° OpenAI ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
        self.temperature = settings.OPENAI_TEMPERATURE
        self.max_tokens = settings.OPENAI_MAX_TOKENS
        self.timeout = settings.OPENAI_TIMEOUT
        
        # Ð¡Ð¸ÑÑ‚ÐµÐ¼Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð¼Ð¿Ñ‚Ñ‹
        self.system_prompts = settings.SYSTEM_PROMPTS
        
        # Ð¡ÐµÐ¼Ð°Ñ„Ð¾Ñ€ Ð´Ð»Ñ Ð¾Ð³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ Ð¾Ð´Ð½Ð¾Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ñ… Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð²
        self.semaphore = asyncio.Semaphore(settings.MAX_CONCURRENT_LLM_REQUESTS)
    
    async def _make_openai_request(
        self,
        messages: List[Dict[str, str]],
        request_type: LLMRequestType,
        **kwargs
    ) -> Dict[str, Any]:
        """Ð‘Ð°Ð·Ð¾Ð²Ñ‹Ð¹ Ð¼ÐµÑ‚Ð¾Ð´ Ð´Ð»Ñ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ðº OpenAI"""
        
        async with self.semaphore:
            start_time = time.time()
            
            try:
                # ÐŸÐ¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ° Ð¿Ð°Ñ€Ð°Ð¼ÐµÑ‚Ñ€Ð¾Ð²
                params = {
                    "model": kwargs.get("model", self.model),
                    "messages": messages,
                    "temperature": kwargs.get("temperature", self.temperature),
                    "max_tokens": kwargs.get("max_tokens", self.max_tokens),
                }
                
                # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
                response = await self.client.chat.completions.create(**params)
                
                # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ð¾Ñ‚Ð²ÐµÑ‚Ð°
                result = {
                    "content": response.choices[0].message.content,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                    },
                    "model": response.model,
                    "latency_ms": int((time.time() - start_time) * 1000)
                }
                
                logger.info(f"LLM Ð·Ð°Ð¿Ñ€Ð¾Ñ {request_type.value} Ð²Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½ ÑƒÑÐ¿ÐµÑˆÐ½Ð¾")
                return result
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° LLM: {error_msg}")
                
                # ÐžÐ±Ñ€Ð°Ð±Ð¾Ñ‚ÐºÐ° Ñ€Ð°Ð·Ð»Ð¸Ñ‡Ð½Ñ‹Ñ… Ñ‚Ð¸Ð¿Ð¾Ð² Ð¾ÑˆÐ¸Ð±Ð¾Ðº
                if "rate_limit" in error_msg.lower() or "429" in error_msg:
                    raise LLMServiceError("ÐŸÑ€ÐµÐ²Ñ‹ÑˆÐµÐ½ Ð»Ð¸Ð¼Ð¸Ñ‚ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ¾Ð² Ðº Ð˜Ð˜. ÐŸÐ¾Ð¿Ñ€Ð¾Ð±ÑƒÐ¹Ñ‚Ðµ Ð¿Ð¾Ð·Ð¶Ðµ.")
                elif "auth" in error_msg.lower() or "401" in error_msg:
                    raise LLMServiceError("ÐžÑˆÐ¸Ð±ÐºÐ° Ð´Ð¾ÑÑ‚ÑƒÐ¿Ð° Ðº ÑÐ»ÑƒÐ¶Ð±Ðµ Ð˜Ð˜")
                elif "api" in error_msg.lower() or "500" in error_msg:
                    raise LLMServiceError("Ð’Ñ€ÐµÐ¼ÐµÐ½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÐ»ÑƒÐ¶Ð±Ñ‹ Ð˜Ð˜")
                else:
                    raise LLMServiceError(f"ÐÐµÐ¾Ð¶Ð¸Ð´Ð°Ð½Ð½Ð°Ñ Ð¾ÑˆÐ¸Ð±ÐºÐ° ÑÐ»ÑƒÐ¶Ð±Ñ‹ Ð˜Ð˜: {error_msg}")
    
    async def chat_with_virtual_trainer(
        self,
        user_message: str,
        chat_history: List[Dict[str, str]] = None,
        user_context: Optional[Dict[str, Any]] = None,
        user_id: str = None,
        session_id: str = None
    ) -> Dict[str, Any]:
        """
        Ð§Ð°Ñ‚ Ñ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ñ‹Ð¼ Ñ‚Ñ€ÐµÐ½ÐµÑ€Ð¾Ð¼
        
        Args:
            user_message: Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
            chat_history: Ð˜ÑÑ‚Ð¾Ñ€Ð¸Ñ Ñ‡Ð°Ñ‚Ð° (Ð¿Ð¾ÑÐ»ÐµÐ´Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ)
            user_context: ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ (Ñ†ÐµÐ»Ð¸, ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ Ð¸ Ñ‚.Ð´.)
        
        Returns:
            ÐžÑ‚Ð²ÐµÑ‚ Ð²Ð¸Ñ€Ñ‚ÑƒÐ°Ð»ÑŒÐ½Ð¾Ð³Ð¾ Ñ‚Ñ€ÐµÐ½ÐµÑ€Ð°
        """
        
        if not user_message.strip():
            raise ValidationError("Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð´ÐµÑ‚Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        context_info = ""
        if user_context:
            # Ð¤Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ñ…Ð°Ñ€Ð°ÐºÑ‚ÐµÑ€Ð¸ÑÑ‚Ð¸ÐºÐ¸
            physical_info = []
            if user_context.get("age"):
                physical_info.append(f"Ð²Ð¾Ð·Ñ€Ð°ÑÑ‚ {user_context['age']} Ð»ÐµÑ‚")
            if user_context.get("gender"):
                physical_info.append(f"Ð¿Ð¾Ð» {user_context['gender']}")
            if user_context.get("height"):
                physical_info.append(f"Ñ€Ð¾ÑÑ‚ {user_context['height']}")
            if user_context.get("weight"):
                physical_info.append(f"Ð²ÐµÑ {user_context['weight']}")
            if physical_info:
                context_info += f"Ð¤Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð´Ð°Ð½Ð½Ñ‹Ðµ: {', '.join(physical_info)}. "
            
            # Ð¤Ð¸Ñ‚Ð½ÐµÑ-Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ
            if user_context.get("goals"):
                if isinstance(user_context['goals'], list):
                    goals_str = ', '.join(user_context['goals'])
                else:
                    goals_str = user_context['goals']
                context_info += f"Ð¦ÐµÐ»Ð¸ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ðº: {goals_str}. "
            
            if user_context.get("fitness_level"):
                context_info += f"Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ¸: {user_context['fitness_level']}. "
            
            if user_context.get("equipment"):
                if isinstance(user_context['equipment'], list):
                    equipment_str = ', '.join(user_context['equipment'])
                else:
                    equipment_str = user_context['equipment']
                context_info += f"Ð”Ð¾ÑÑ‚ÑƒÐ¿Ð½Ð¾Ðµ Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ: {equipment_str}. "
            
            if user_context.get("limitations"):
                if isinstance(user_context['limitations'], list):
                    limitations_str = ', '.join(user_context['limitations'])
                else:
                    limitations_str = user_context['limitations']
                context_info += f"ÐžÐ³Ñ€Ð°Ð½Ð¸Ñ‡ÐµÐ½Ð¸Ñ: {limitations_str}. "
            
            # ÐŸÐ¸Ñ‚Ð°Ð½Ð¸Ðµ
            nutrition_info = []
            if user_context.get("nutrition_goal"):
                nutrition_info.append(f"Ñ†ÐµÐ»ÑŒ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ: {user_context['nutrition_goal']}")
            if user_context.get("food_preferences"):
                if isinstance(user_context['food_preferences'], list):
                    prefs_str = ', '.join(user_context['food_preferences'])
                else:
                    prefs_str = user_context['food_preferences']
                nutrition_info.append(f"Ð¿Ñ€ÐµÐ´Ð¿Ð¾Ñ‡Ñ‚ÐµÐ½Ð¸Ñ: {prefs_str}")
            if user_context.get("allergies"):
                if isinstance(user_context['allergies'], list):
                    allergies_str = ', '.join(user_context['allergies'])
                else:
                    allergies_str = user_context['allergies']
                nutrition_info.append(f"Ð°Ð»Ð»ÐµÑ€Ð³Ð¸Ð¸: {allergies_str}")
            if nutrition_info:
                context_info += f"ÐŸÐ¸Ñ‚Ð°Ð½Ð¸Ðµ: {', '.join(nutrition_info)}. "
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["virtual_trainer"] + 
                          (f"\n\nÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°: {context_info}" if context_info else "")
            }
        ]
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ‡Ð°Ñ‚Ð°
        if chat_history:
            for msg in chat_history[-settings.MAX_CHAT_HISTORY:]:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        # Ð”Ð¾Ð±Ð°Ð²Ð»ÐµÐ½Ð¸Ðµ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        # Store conversation in knowledge base
        if user_id and session_id:
            try:
                # Store user message
                await knowledge_base.store_conversation_message(
                    user_id=user_id,
                    session_id=session_id,
                    role="user",
                    content=user_message,
                    importance_score=1.0
                )
            except Exception as e:
                logger.warning(f"Failed to store user message in knowledge base: {e}")

        # Universal approach: Always provide RAG tools, let LLM decide when to use them
        if user_id and session_id:
            logger.info("Using RAG-enhanced chat with LLM decision-making")
            
            # Always use RAG tools, let AI decide when to call them
            final_result = await self._chat_with_rag_tools(
                messages=messages,
                user_id=user_id,
                session_id=session_id,
                user_message=user_message
            )
            
            if not final_result:
                # Fallback to normal chat if RAG fails
                logger.warning("RAG-enhanced chat failed, falling back to normal chat")
                final_result = await self._make_openai_request(
                    messages=messages,
                    request_type=LLMRequestType.CHAT
                )
                final_result["used_rag"] = False
        else:
            # Normal chat without RAG (no user context)
            logger.info("Using normal chat (no user context)")
            final_result = await self._make_openai_request(
                messages=messages,
                request_type=LLMRequestType.CHAT
            )
            final_result["used_rag"] = False

        # Store AI response in knowledge base
        if user_id and session_id:
            try:
                await knowledge_base.store_conversation_message(
                    user_id=user_id,
                    session_id=session_id,
                    role="assistant",
                    content=final_result["content"],
                    importance_score=1.2 if final_result.get("used_rag", False) else 1.0
                )
            except Exception as e:
                logger.warning(f"Failed to store AI message in knowledge base: {e}")

        return {
            "response": final_result["content"],
            "used_rag": final_result.get("used_rag", False),
            "metadata": {
                "tokens_used": final_result["usage"]["total_tokens"],
                "model": final_result["model"],
                "latency_ms": final_result["latency_ms"]
            }
        }
    
    async def _should_use_rag_tools(self, user_message: str, chat_history: List[Dict[str, str]] = None) -> bool:
        """
        DEPRECATED: This function is no longer used.
        RAG tools are now always available and the LLM decides when to use them.
        """
        # This function is kept for backward compatibility but not used
        return True
    
    async def _chat_with_rag_tools(
        self,
        messages: List[Dict[str, str]],
        user_id: str,
        session_id: str,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Enhanced chat that provides RAG tools for AI to use when needed
        
        Args:
            messages: Chat messages to send
            user_id: User ID for RAG filtering
            session_id: Session ID for RAG filtering  
            user_message: Current user message
            
        Returns:
            OpenAI response with RAG enhancement tracking
        """
        
        try:
            # Add RAG tools to the request
            tools = rag_tools.get_tool_definitions()
            
            # Enhanced system message for intelligent RAG usage
            rag_system_message = """
            
            ðŸ§  Ð˜ÐÐ¡Ð¢Ð Ð£ÐœÐ•ÐÐ¢Ð« ÐŸÐÐœÐ¯Ð¢Ð˜:
            Ð£ Ñ‚ÐµÐ±Ñ ÐµÑÑ‚ÑŒ Ð´Ð¾ÑÑ‚ÑƒÐ¿ Ðº Ð¿Ð¾Ð»Ð½Ð¾Ð¹ Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð¾Ð² Ñ ÑÑ‚Ð¸Ð¼ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÐµÐ¼. Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÑÑ‚Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹ Ñ€Ð°Ð·ÑƒÐ¼Ð½Ð¾:
            
            ðŸ“‹ search_conversation_history - Ð½Ð°Ð¹Ñ‚Ð¸ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ Ð¸Ð· Ð¿Ñ€Ð¾ÑˆÐ»Ñ‹Ñ… Ð±ÐµÑÐµÐ´
            ðŸ“Š get_conversation_summary - Ð¿Ð¾Ð»ÑƒÑ‡Ð¸Ñ‚ÑŒ Ð¾Ð±Ð·Ð¾Ñ€ Ð½ÐµÐ´Ð°Ð²Ð½Ð¸Ñ… Ñ‚ÐµÐ¼ Ð¸ Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ð¹  
            ðŸ” find_related_discussions - Ð½Ð°Ð¹Ñ‚Ð¸ Ð²ÑÐµ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ð¾Ð±ÑÑƒÐ¶Ð´ÐµÐ½Ð¸Ñ Ð¿Ð¾ Ñ‚ÐµÐ¼Ðµ
            
            ÐšÐžÐ“Ð”Ð Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð¬:
            âœ… ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÑÑ‹Ð»Ð°ÐµÑ‚ÑÑ Ð½Ð° Ð¿Ñ€Ð¾ÑˆÐ»Ñ‹Ðµ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ñ‹ ("Ð¿Ð¾Ð¼Ð½Ð¸ÑˆÑŒ", "Ð¼Ñ‹ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð»Ð¸", "Ñ‚Ñ‹ Ð³Ð¾Ð²Ð¾Ñ€Ð¸Ð»")
            âœ… Ð¡Ð¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ð¾ ÑÐ²Ð¾ÐµÐ¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ðµ, Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ, Ñ†ÐµÐ»ÑÑ… ("Ð¼Ð¾Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°", "ÐºÐ°Ðº Ð´ÐµÐ»Ð° Ñ...")
            âœ… Ð’Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ñ‚Ñ€ÐµÐ±ÑƒÑŽÑ‚ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»ÑŒÐ½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð° ("ÐºÐ¾Ð³Ð´Ð° Ñ...", "Ñ‡Ñ‚Ð¾ Ñ...")
            âœ… ÐÑƒÐ¶Ð½Ð° Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¾ Ð¿Ñ€ÐµÐ´Ñ‹Ð´ÑƒÑ‰Ð¸Ñ… Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑÑ… Ð¸Ð»Ð¸ Ð¿Ð»Ð°Ð½Ð°Ñ…
            âœ… ÐŸÐ¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ Ð¾ Ð²Ñ€ÐµÐ¼ÐµÐ½Ð¸/Ð´Ð°Ñ‚Ð°Ñ… ÑÐ¾Ð±Ñ‹Ñ‚Ð¸Ð¹ ("ÐºÐ¾Ð³Ð´Ð°", "Ð²Ñ‡ÐµÑ€Ð°", "Ð½Ð° Ð¿Ñ€Ð¾ÑˆÐ»Ð¾Ð¹ Ð½ÐµÐ´ÐµÐ»Ðµ")
            
            ÐšÐžÐ“Ð”Ð ÐÐ• Ð˜Ð¡ÐŸÐžÐ›Ð¬Ð—ÐžÐ’ÐÐ¢Ð¬:
            âŒ ÐžÐ±Ñ‰Ð¸Ðµ Ð²Ð¾Ð¿Ñ€Ð¾ÑÑ‹ Ð¾ Ñ„Ð¸Ñ‚Ð½ÐµÑÐµ/Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ð¸ Ð±ÐµÐ· Ð»Ð¸Ñ‡Ð½Ð¾Ð³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
            âŒ ÐÐ¾Ð²Ñ‹Ðµ Ñ‚ÐµÐ¼Ñ‹, Ð½Ðµ ÑÐ²ÑÐ·Ð°Ð½Ð½Ñ‹Ðµ Ñ Ð¸ÑÑ‚Ð¾Ñ€Ð¸ÐµÐ¹
            âŒ ÐŸÑ€Ð¾ÑÑ‚Ñ‹Ðµ Ð¿Ñ€Ð¸Ð²ÐµÑ‚ÑÑ‚Ð²Ð¸Ñ Ð¸Ð»Ð¸ Ð±Ð»Ð°Ð³Ð¾Ð´Ð°Ñ€Ð½Ð¾ÑÑ‚Ð¸
            âŒ Ð’Ð¾Ð¿Ñ€Ð¾ÑÑ‹, Ð½Ð° ÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¼Ð¾Ð¶ÐµÑˆÑŒ Ð¾Ñ‚Ð²ÐµÑ‚Ð¸Ñ‚ÑŒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ñ‚ÐµÐºÑƒÑ‰ÐµÐ³Ð¾ ÐºÐ¾Ð½Ñ‚ÐµÐºÑÑ‚Ð°
            
            ðŸŽ¯ ÐšÐ Ð˜Ð¢Ð˜Ð§Ð•Ð¡ÐšÐ˜ Ð’ÐÐ–ÐÐž - ÐšÐÐš Ð˜ÐÐ¢Ð•Ð ÐŸÐ Ð•Ð¢Ð˜Ð ÐžÐ’ÐÐ¢Ð¬ Ð Ð•Ð—Ð£Ð›Ð¬Ð¢ÐÐ¢Ð« Ð˜ÐÐ¡Ð¢Ð Ð£ÐœÐ•ÐÐ¢ÐžÐ’:
            
            ÐšÐ¾Ð³Ð´Ð° Ñ‚Ñ‹ Ð¿Ð¾Ð»ÑƒÑ‡Ð°ÐµÑˆÑŒ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ð¾Ñ‚ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ð¾Ð² Ð¿Ð°Ð¼ÑÑ‚Ð¸, Ð’Ð¡Ð•Ð“Ð”Ð Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¸Ñ… ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ:
            
            âœ… Ð•ÑÐ»Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ Ð¿Ð¾Ð»ÐµÐ¼ "results" Ð¸ Ñ‚Ð°Ð¼ ÐµÑÑ‚ÑŒ Ð·Ð°Ð¿Ð¸ÑÐ¸ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð¾Ð² - Ð­Ð¢Ðž ÐžÐ—ÐÐÐ§ÐÐ•Ð¢, Ð§Ð¢Ðž Ð˜ÐÐ¤ÐžÐ ÐœÐÐ¦Ð˜Ð¯ ÐÐÐ™Ð”Ð•ÐÐ!
            âœ… Ð”Ð°Ð¶Ðµ ÐµÑÐ»Ð¸ Ð½Ð°Ð¹Ð´ÐµÐ½Ð¾ Ð¼Ð½Ð¾Ð³Ð¾ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð¾Ð², Ð²Ð½Ð¸Ð¼Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾ Ð¸Ñ‰Ð¸ ÑÑ€ÐµÐ´Ð¸ Ð½Ð¸Ñ… Ð½ÑƒÐ¶Ð½ÑƒÑŽ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸ÑŽ
            âœ… ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð¿Ð¾Ð»Ñ "content" Ð² ÐºÐ°Ð¶Ð´Ð¾Ð¼ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ðµ
            âœ… ÐžÐ±Ñ€Ð°Ñ‰Ð°Ð¹ Ð²Ð½Ð¸Ð¼Ð°Ð½Ð¸Ðµ Ð½Ð° Ð²Ñ€ÐµÐ¼ÐµÐ½Ð½Ñ‹Ðµ Ð¼ÐµÑ‚ÐºÐ¸ (timestamp) Ð´Ð»Ñ Ð¿Ð¾Ð½Ð¸Ð¼Ð°Ð½Ð¸Ñ Ñ…Ñ€Ð¾Ð½Ð¾Ð»Ð¾Ð³Ð¸Ð¸
            âœ… Ð•ÑÐ»Ð¸ Ð² Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ð°Ñ… ÐµÑÑ‚ÑŒ Ñ€ÐµÐ»ÐµÐ²Ð°Ð½Ñ‚Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ - Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ ÐµÑ‘ Ð² Ð¾Ñ‚Ð²ÐµÑ‚Ðµ, Ð½Ðµ Ð³Ð¾Ð²Ð¾Ñ€Ð¸ Ñ‡Ñ‚Ð¾ Ð½Ðµ Ð½Ð°ÑˆÐµÐ»!
            
            âŒ ÐÐ• Ð“ÐžÐ’ÐžÐ Ð˜ "Ð½Ðµ Ð½Ð°ÑˆÐµÐ» Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸" ÐµÑÐ»Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð²ÐµÑ€Ð½ÑƒÐ» Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹!
            âŒ ÐÐ• Ð˜Ð“ÐÐžÐ Ð˜Ð Ð£Ð™ ÑÐ¾Ð´ÐµÑ€Ð¶Ð¸Ð¼Ð¾Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ñ‹Ñ… Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð¾Ð²!
            âŒ ÐÐ• Ð´Ð°Ð²Ð°Ð¹ Ð¾Ð±Ñ‰Ð¸Ðµ Ð¾Ñ‚Ð²ÐµÑ‚Ñ‹ ÐµÑÐ»Ð¸ ÐµÑÑ‚ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð°Ñ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¸Ð· Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸!
            
            ÐŸÐ Ð˜ÐœÐ•Ð : Ð•ÑÐ»Ð¸ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»ÑŒ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°ÐµÑ‚ "ÐšÐ¾Ð³Ð´Ð° Ñ ÑÐ¿Ñ€Ð°ÑˆÐ¸Ð²Ð°Ð» Ð¿Ñ€Ð¾ Ð²Ð¾Ð´Ñƒ?" Ð¸ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚ Ð²Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€ÐµÐ·ÑƒÐ»ÑŒÑ‚Ð°Ñ‚Ñ‹ Ñ Ñ€Ð°Ð·Ð³Ð¾Ð²Ð¾Ñ€Ð°Ð¼Ð¸ Ð¾ Ð²Ð¾Ð´Ðµ - Ð¾Ñ‚Ð²ÐµÑ‚ÑŒ ÐºÐ¾Ð½ÐºÑ€ÐµÑ‚Ð½Ð¾ Ð¾ÑÐ½Ð¾Ð²Ñ‹Ð²Ð°ÑÑÑŒ Ð½Ð° Ð½Ð°Ð¹Ð´ÐµÐ½Ð½Ð¾Ð¹ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ð¸, ÑƒÐºÐ°Ð¶Ð¸ Ð´Ð°Ñ‚Ñƒ/Ð²Ñ€ÐµÐ¼Ñ Ð¸ Ð¿ÐµÑ€ÐµÑÐºÐ°Ð¶Ð¸ Ñ‡Ñ‚Ð¾ Ð¾Ð±ÑÑƒÐ¶Ð´Ð°Ð»Ð¾ÑÑŒ.
            
            ÐŸÑ€Ð¸Ð½Ð¸Ð¼Ð°Ð¹ Ñ€ÐµÑˆÐµÐ½Ð¸Ðµ ÑÐ°Ð¼Ð¾ÑÑ‚Ð¾ÑÑ‚ÐµÐ»ÑŒÐ½Ð¾ - ÐµÑÐ»Ð¸ ÑÑ‡Ð¸Ñ‚Ð°ÐµÑˆÑŒ Ñ‡Ñ‚Ð¾ Ð¸Ð½Ñ„Ð¾Ñ€Ð¼Ð°Ñ†Ð¸Ñ Ð¸Ð· Ð¸ÑÑ‚Ð¾Ñ€Ð¸Ð¸ Ð¿Ð¾Ð¼Ð¾Ð¶ÐµÑ‚ Ð´Ð°Ñ‚ÑŒ Ð»ÑƒÑ‡ÑˆÐ¸Ð¹ Ð¾Ñ‚Ð²ÐµÑ‚, Ð¸ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐ¹ Ð¸Ð½ÑÑ‚Ñ€ÑƒÐ¼ÐµÐ½Ñ‚Ñ‹.
            """
            
            # Modify the first system message to include RAG instructions
            if messages and messages[0]["role"] == "system":
                messages[0]["content"] = messages[0]["content"] + rag_system_message
            
            # Make request with tools
            response = await self._make_openai_request_with_tools(
                messages=messages,
                tools=tools,
                user_id=user_id,
                session_id=session_id
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error in RAG-enhanced chat: {e}")
            return None
    
    async def _make_openai_request_with_tools(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict[str, Any]],
        user_id: str,
        session_id: str
    ) -> Dict[str, Any]:
        """
        Make OpenAI request with function calling tools
        
        Args:
            messages: Chat messages
            tools: Available tools for function calling
            user_id: User ID for tool execution
            session_id: Session ID for tool execution
            
        Returns:
            Final response after tool execution
        """
        
        start_time = time.time()
        total_tokens = 0
        
        try:
            # Initial request with tools
            response = await self.client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                tools=tools,
                tool_choice="auto",
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=settings.OPENAI_MAX_TOKENS,
                timeout=settings.OPENAI_TIMEOUT
            )
            
            total_tokens += response.usage.total_tokens
            
            # Check if AI wants to use tools
            if response.choices[0].message.tool_calls:
                logger.info(f"AI requested {len(response.choices[0].message.tool_calls)} tool calls")
                
                # Add AI message with tool calls to conversation
                messages.append({
                    "role": "assistant",
                    "content": response.choices[0].message.content,
                    "tool_calls": [
                        {
                            "id": tool_call.id,
                            "type": tool_call.type,
                            "function": {
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments
                            }
                        }
                        for tool_call in response.choices[0].message.tool_calls
                    ]
                })
                
                # Execute each tool call
                for tool_call in response.choices[0].message.tool_calls:
                    try:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        logger.info(f"Executing tool: {function_name} with args: {function_args}")
                        
                        # Execute the tool
                        tool_result = await rag_tools.execute_tool(
                            tool_name=function_name,
                            tool_arguments=function_args,
                            user_id=user_id,
                            session_id=session_id,
                            current_session_context=messages  # Pass current conversation context
                        )
                        
                        # Add tool result to conversation
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(tool_result, ensure_ascii=False)
                        })
                        
                    except Exception as e:
                        logger.error(f"Error executing tool {tool_call.function.name}: {e}")
                        # Add error message
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                        })
                
                # Get final response after tool execution
                final_response = await self.client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=messages,
                    temperature=settings.OPENAI_TEMPERATURE,
                    max_tokens=settings.OPENAI_MAX_TOKENS,
                    timeout=settings.OPENAI_TIMEOUT
                )
                
                total_tokens += final_response.usage.total_tokens
                
                end_time = time.time()
                latency_ms = int((end_time - start_time) * 1000)
                
                return {
                    "content": final_response.choices[0].message.content,
                    "usage": {"total_tokens": total_tokens},
                    "model": final_response.model,
                    "latency_ms": latency_ms,
                    "used_rag": True
                }
            
            else:
                # No tools used, return direct response
                end_time = time.time()
                latency_ms = int((end_time - start_time) * 1000)
                
                return {
                    "content": response.choices[0].message.content,
                    "usage": {"total_tokens": total_tokens},
                    "model": response.model,
                    "latency_ms": latency_ms,
                    "used_rag": False
                }
                
        except Exception as e:
            logger.error(f"Error in OpenAI request with tools: {e}")
            raise LLMServiceError(f"Tool-enhanced chat failed: {str(e)}")
    
    def _create_fallback_workout_program(self, client_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a basic fallback workout program if LLM fails"""
        goal = client_data.get("goal", "Ð¾Ð±Ñ‰Ð°Ñ Ñ„Ð¸Ð·Ð¸Ñ‡ÐµÑÐºÐ°Ñ Ð¿Ð¾Ð´Ð³Ð¾Ñ‚Ð¾Ð²ÐºÐ°")
        level = client_data.get("level", "Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹")
        equipment = client_data.get("equipment", ["ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð²ÐµÑ"])
        
        # Basic program structure
        basic_exercises = {
            "upper_body": ["ÐžÑ‚Ð¶Ð¸Ð¼Ð°Ð½Ð¸Ñ", "ÐŸÐ»Ð°Ð½ÐºÐ°", "ÐŸÐ¾Ð´Ñ‚ÑÐ³Ð¸Ð²Ð°Ð½Ð¸Ñ"],
            "lower_body": ["ÐŸÑ€Ð¸ÑÐµÐ´Ð°Ð½Ð¸Ñ", "Ð’Ñ‹Ð¿Ð°Ð´Ñ‹", "ÐŸÐ¾Ð´ÑŠÐµÐ¼Ñ‹ Ð½Ð° Ð½Ð¾ÑÐºÐ¸"],
            "cardio": ["Ð‘ÐµÐ³ Ð½Ð° Ð¼ÐµÑÑ‚Ðµ", "ÐŸÑ€Ñ‹Ð¶ÐºÐ¸", "Ð‘ÐµÑ€Ð¿Ð¸"]
        }
        
        # Adjust difficulty based on level
        if "Ð½Ð°Ñ‡Ð°Ð»ÑŒÐ½Ñ‹Ð¹" in level.lower():
            sets, reps = 2, "8-10"
        elif "ÑÑ€ÐµÐ´Ð½Ð¸Ð¹" in level.lower():
            sets, reps = 3, "10-12"
        else:  # advanced
            sets, reps = 4, "12-15"
        
        weeks = []
        for week_num in range(1, 5):  # 4 weeks
            workouts = []
            
            # 3 workouts per week
            workout_types = ["Ð’ÐµÑ€Ñ… Ñ‚ÐµÐ»Ð°", "ÐÐ¸Ð· Ñ‚ÐµÐ»Ð°", "ÐšÐ°Ñ€Ð´Ð¸Ð¾"]
            exercise_groups = ["upper_body", "lower_body", "cardio"]
            
            for i, (workout_type, exercise_group) in enumerate(zip(workout_types, exercise_groups)):
                exercises = []
                for exercise_name in basic_exercises[exercise_group]:
                    exercises.append({
                        "name": exercise_name,
                        "sets": sets,
                        "reps": reps,
                        "weight": 0,
                        "notes": ""
                    })
                
                workouts.append({
                    "name": workout_type,
                    "exercises": exercises
                })
            
            weeks.append({
                "week_number": week_num,
                "workouts": workouts
            })
        
        return {
            "goal": goal,
            "level": level,
            "duration_weeks": 4,
            "workouts_per_week": 3,
            "equipment": equipment,
            "weeks": weeks
        }

    async def generate_workout_program(
        self,
        client_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹
        
        Args:
            client_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° (Ñ†ÐµÐ»Ð¸, ÑƒÑ€Ð¾Ð²ÐµÐ½ÑŒ, Ð¾Ð±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ Ð¸ Ñ‚.Ð´.)
        
        Returns:
            Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð° Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ðº Ð² JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ
        """
        
        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        required_fields = ["goal", "level", "sessions_per_week"]
        for field in required_fields:
            if field not in client_data:
                raise ValidationError(f"ÐžÑ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð»Ðµ: {field}")
        
        try:
            # Try LLM generation first
            equipment_str = ", ".join(client_data.get("equipment", ["ÑÐ¾Ð±ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ð¹ Ð²ÐµÑ"]))
            limitations_str = ", ".join(client_data.get("limitations", ["Ð½ÐµÑ‚"]))
            
            user_prompt = f"""
            Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½ÑƒÑŽ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ð½Ð° 4 Ð½ÐµÐ´ÐµÐ»Ð¸:
            Ð¦ÐµÐ»ÑŒ: {client_data["goal"]}
            Ð£Ñ€Ð¾Ð²ÐµÐ½ÑŒ: {client_data["level"]}
            ÐžÐ±Ð¾Ñ€ÑƒÐ´Ð¾Ð²Ð°Ð½Ð¸Ðµ: {equipment_str}
            
            JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚:
            {{
                "goal": "{client_data['goal']}",
                "level": "{client_data['level']}",
                "duration_weeks": 4,
                "workouts_per_week": 3,
                "equipment": ["{equipment_str}"],
                "weeks": [
                    {{
                        "week_number": 1,
                        "workouts": [
                            {{
                                "name": "Ð¢Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ° 1",
                                "exercises": [
                                    {{"name": "ÐŸÑ€Ð¸ÑÐµÐ´Ð°Ð½Ð¸Ñ", "sets": 3, "reps": "10-12", "weight": 0}}
                                ]
                            }}
                        ]
                    }}
                ]
            }}
            """
            
            messages = [
                {
                    "role": "system", 
                    "content": "Ð¢Ñ‹ Ñ„Ð¸Ñ‚Ð½ÐµÑ-Ñ‚Ñ€ÐµÐ½ÐµÑ€. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON."
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
            
            # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
            result = await self._make_openai_request(
                messages=messages,
                request_type=LLMRequestType.PROGRAM_CREATE,
                max_tokens=1500  # Reduced for simpler response
            )
            
            # ÐŸÐ°Ñ€ÑÐ¸Ð½Ð³ JSON
            try:
                # Clean response
                cleaned_content = self._clean_json_response(result["content"])
                program_data = json.loads(cleaned_content)
                
                # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñ‹
                if "weeks" not in program_data:
                    raise ValueError("ÐžÑ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð° weeks")
                
                return {
                    "program": program_data,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "llm"
                    }
                }
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLM Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ðµ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON: {e}")
                logger.info("Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ðº")
                
                # Use fallback program
                fallback_program = self._create_fallback_workout_program(client_data)
                
                return {
                    "program": fallback_program,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "fallback"
                    }
                }
        
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ðº: {e}")
            logger.info("Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ðº")
            
            # Create fallback program
            fallback_program = self._create_fallback_workout_program(client_data)
            
            return {
                "program": fallback_program,
                "metadata": {
                    "tokens_used": 0,
                    "model": "fallback",
                    "latency_ms": 0,
                    "source": "fallback"
                }
            }
    
    async def adjust_workout_program(
        self,
        current_program: Dict[str, Any],
        feedback: str,
        progress_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        ÐšÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²ÐºÐ° Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹
        
        Args:
            current_program: Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°
            feedback: ÐžÐ±Ñ€Ð°Ñ‚Ð½Ð°Ñ ÑÐ²ÑÐ·ÑŒ Ð¾Ñ‚ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
            progress_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ
        
        Returns:
            ÐžÐ±Ð½Ð¾Ð²Ð»ÐµÐ½Ð½Ð°Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°
        """
        
        if not feedback.strip():
            raise ValidationError("ÐžÐ±Ñ€Ð°Ñ‚Ð½Ð°Ñ ÑÐ²ÑÐ·ÑŒ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ð¾Ð¹")
        
        progress_info = ""
        if progress_data:
            progress_info = f"\nÐ”Ð°Ð½Ð½Ñ‹Ðµ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: {json.dumps(progress_data, ensure_ascii=False)}"
        
        user_prompt = f"""
        Ð¡ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€ÑƒÐ¹ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²Ð¾Ñ‡Ð½ÑƒÑŽ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¾Ð±Ñ€Ð°Ñ‚Ð½Ð¾Ð¹ ÑÐ²ÑÐ·Ð¸.
        
        Ð¢ÐµÐºÑƒÑ‰Ð°Ñ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ð°:
        {json.dumps(current_program, ensure_ascii=False, indent=2)}
        
        ÐžÐ±Ñ€Ð°Ñ‚Ð½Ð°Ñ ÑÐ²ÑÐ·ÑŒ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°: {feedback}
        {progress_info}
        
        Ð’ÐµÑ€Ð½Ð¸ ÐžÐ‘ÐÐžÐ’Ð›Ð•ÐÐÐ£Ð® Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñƒ Ð² Ñ‚Ð¾Ð¼ Ð¶Ðµ JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ, Ð½Ð¾ Ñ ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²ÐºÐ°Ð¼Ð¸.
        Ð¡Ð¾Ñ…Ñ€Ð°Ð½Ð¸ Ð¾Ð±Ñ‰ÑƒÑŽ ÑÑ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ñƒ, Ð½Ð¾ Ð°Ð´Ð°Ð¿Ñ‚Ð¸Ñ€ÑƒÐ¹ Ð½Ð°Ð³Ñ€ÑƒÐ·ÐºÑƒ, ÑƒÐ¿Ñ€Ð°Ð¶Ð½ÐµÐ½Ð¸Ñ Ð¸Ð»Ð¸ Ð¿Ð¾Ð´Ñ…Ð¾Ð´Ñ‹.
        """
        
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["program_generator"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.PROGRAM_ADJUST,
            max_tokens=2000
        )
        
        try:
            adjusted_program = json.loads(result["content"])
            
            return {
                "program": adjusted_program,
                "metadata": {
                    "tokens_used": result["usage"]["total_tokens"],
                    "model": result["model"],
                    "latency_ms": result["latency_ms"]
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÑÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð¹ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹: {e}")
            raise LLMServiceError("ÐžÑˆÐ¸Ð±ÐºÐ° ÐºÐ¾Ñ€Ñ€ÐµÐºÑ‚Ð¸Ñ€Ð¾Ð²ÐºÐ¸ Ð¿Ñ€Ð¾Ð³Ñ€Ð°Ð¼Ð¼Ñ‹")
    
    async def analyze_progress(
        self,
        client_data: Dict[str, Any],
        period_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        ÐÐ½Ð°Ð»Ð¸Ð· Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ° ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
        
        Args:
            client_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
            period_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´ (Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐ¸, Ð·Ð°Ð¼ÐµÑ€Ñ‹)
        
        Returns:
            ÐžÑ‚Ñ‡ÐµÑ‚ Ñ Ð°Ð½Ð°Ð»Ð¸Ð·Ð¾Ð¼ Ð¸ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑÐ¼Ð¸
        """
        
        user_prompt = f"""
        ÐŸÑ€Ð¾Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð° Ð¸ Ð´Ð°Ð¹ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ð¸.
        
        Ð”Ð°Ð½Ð½Ñ‹Ðµ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°:
        {json.dumps(client_data, ensure_ascii=False, indent=2)}
        
        Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð·Ð° Ð¿ÐµÑ€Ð¸Ð¾Ð´:
        {json.dumps(period_data, ensure_ascii=False, indent=2)}
        
        Ð’ÐµÑ€Ð½Ð¸ Ð°Ð½Ð°Ð»Ð¸Ð· Ð² JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ:
        {{
            "summary": "ÐšÑ€Ð°Ñ‚ÐºÐ¸Ð¹ Ð¸Ñ‚Ð¾Ð³ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°",
            "bottlenecks": ["Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° 1", "Ð¿Ñ€Ð¾Ð±Ð»ÐµÐ¼Ð° 2"],
            "recommendations": ["Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ 1", "Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸Ñ 2"],
            "achievements": ["Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ðµ 1", "Ð´Ð¾ÑÑ‚Ð¸Ð¶ÐµÐ½Ð¸Ðµ 2"],
            "next_goals": ["Ñ†ÐµÐ»ÑŒ 1", "Ñ†ÐµÐ»ÑŒ 2"]
        }}
        """
        
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["progress_analyzer"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.PROGRESS_ANALYZE
        )
        
        try:
            analysis = json.loads(result["content"])
            
            return {
                "analysis": analysis,
                "metadata": {
                    "tokens_used": result["usage"]["total_tokens"],
                    "model": result["model"],
                    "latency_ms": result["latency_ms"]
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°: {e}")
            raise LLMServiceError("ÐžÑˆÐ¸Ð±ÐºÐ° Ð°Ð½Ð°Ð»Ð¸Ð·Ð° Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐ°")
    
    async def generate_notification(
        self,
        notification_type: str,
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ð³Ð¾ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ
        
        Args:
            notification_type: Ð¢Ð¸Ð¿ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ñ
            user_context: ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ
        
        Returns:
            ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ
        """
        
        type_prompts = {
            "workout_reminder": "Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¼Ð¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÑŽÑ‰ÐµÐµ Ð½Ð°Ð¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ðµ Ð¾ Ð¿Ñ€ÐµÐ´ÑÑ‚Ð¾ÑÑ‰ÐµÐ¹ Ñ‚Ñ€ÐµÐ½Ð¸Ñ€Ð¾Ð²ÐºÐµ",
            "payment_reminder": "Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð²ÐµÐ¶Ð»Ð¸Ð²Ð¾Ðµ Ð½Ð°Ð¿Ð¾Ð¼Ð¸Ð½Ð°Ð½Ð¸Ðµ Ð¾Ð± Ð¾Ð¿Ð»Ð°Ñ‚Ðµ Ð°Ð±Ð¾Ð½ÐµÐ¼ÐµÐ½Ñ‚Ð°",
            "motivational": "Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¼Ð¾Ñ‚Ð¸Ð²Ð°Ñ†Ð¸Ð¾Ð½Ð½Ð¾Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð»Ñ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°",
            "progress_report": "Ð¡Ð¾Ð·Ð´Ð°Ð¹ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ñ Ð¿Ñ€Ð¸Ð³Ð»Ð°ÑˆÐµÐ½Ð¸ÐµÐ¼ Ð¿Ð¾ÑÐ¼Ð¾Ñ‚Ñ€ÐµÑ‚ÑŒ Ð¾Ñ‚Ñ‡ÐµÑ‚ Ð¾ Ð¿Ñ€Ð¾Ð³Ñ€ÐµÑÑÐµ"
        }
        
        specific_prompt = type_prompts.get(notification_type, "Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¿ÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ð¾Ðµ ÑƒÐ²ÐµÐ´Ð¾Ð¼Ð»ÐµÐ½Ð¸Ðµ")
        
        user_prompt = f"""
        {specific_prompt}.
        
        ÐšÐ¾Ð½Ñ‚ÐµÐºÑÑ‚ Ð¿Ð¾Ð»ÑŒÐ·Ð¾Ð²Ð°Ñ‚ÐµÐ»Ñ:
        {json.dumps(user_context, ensure_ascii=False, indent=2)}
        
        Ð¡Ð¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ðµ Ð´Ð¾Ð»Ð¶Ð½Ð¾ Ð±Ñ‹Ñ‚ÑŒ:
        - ÐŸÐµÑ€ÑÐ¾Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¼
        - Ð”Ñ€ÑƒÐ¶ÐµÐ»ÑŽÐ±Ð½Ñ‹Ð¼ Ð½Ð¾ Ð½Ðµ Ð½Ð°Ð²ÑÐ·Ñ‡Ð¸Ð²Ñ‹Ð¼
        - ÐœÐ¾Ñ‚Ð¸Ð²Ð¸Ñ€ÑƒÑŽÑ‰Ð¸Ð¼
        - ÐÐ° Ñ€ÑƒÑÑÐºÐ¾Ð¼ ÑÐ·Ñ‹ÐºÐµ
        - Ð”Ð»Ð¸Ð½Ð¾Ð¹ Ð½Ðµ Ð±Ð¾Ð»ÐµÐµ 200 ÑÐ¸Ð¼Ð²Ð¾Ð»Ð¾Ð²
        
        Ð’ÐµÑ€Ð½Ð¸ Ñ‚Ð¾Ð»ÑŒÐºÐ¾ Ñ‚ÐµÐºÑÑ‚ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ñ Ð±ÐµÐ· Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ð³Ð¾ Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ.
        """
        
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["notification_creator"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.NOTIFICATION_CREATE,
            max_tokens=100
        )
        
        return {
            "message": result["content"].strip(),
            "metadata": {
                "tokens_used": result["usage"]["total_tokens"],
                "model": result["model"],
                "latency_ms": result["latency_ms"]
            }
        }

    def _clean_json_response(self, content: str) -> str:
        """Clean and fix common JSON issues in LLM responses"""
        content = content.strip()
        
        # Remove markdown code blocks if present
        if content.startswith('```json'):
            content = content[7:]
        if content.startswith('```'):
            content = content[3:]
        if content.endswith('```'):
            content = content[:-3]
        
        # Find JSON block
        json_start = content.find('{')
        json_end = content.rfind('}')
        
        if json_start >= 0 and json_end > json_start:
            content = content[json_start:json_end+1]
        
        # Fix common issues
        content = content.replace("'", '"')  # Single quotes to double quotes
        content = content.replace(',,', ',')  # Remove double commas
        content = content.replace(',}', '}')  # Remove trailing commas before }
        content = content.replace(',]', ']')  # Remove trailing commas before ]
        
        return content

    def _create_fallback_nutrition_plan(self, nutrition_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a basic fallback nutrition plan if LLM fails"""
        daily_calories = nutrition_data["daily_calories"]
        goal = nutrition_data["nutrition_goal"]
        
        # Calculate macros
        if "Ð¿Ð¾Ñ…ÑƒÐ´ÐµÐ½Ð¸Ðµ" in goal.lower():
            protein_ratio, fat_ratio, carb_ratio = 0.35, 0.30, 0.35
        elif "Ð½Ð°Ð±Ð¾Ñ€" in goal.lower():
            protein_ratio, fat_ratio, carb_ratio = 0.30, 0.25, 0.45
        else:
            protein_ratio, fat_ratio, carb_ratio = 0.30, 0.30, 0.40
        
        daily_protein = int((daily_calories * protein_ratio) / 4)
        daily_fats = int((daily_calories * fat_ratio) / 9)
        daily_carbs = int((daily_calories * carb_ratio) / 4)
        
        # Basic meal plan
        meal_calories = daily_calories // 4
        
        return {
            "goal": goal,
            "daily_calories": daily_calories,
            "daily_protein": daily_protein,
            "daily_fats": daily_fats,
            "daily_carbs": daily_carbs,
            "days": [
                {
                    "day_name": day,
                    "meals": [
                        {
                            "name": "Ð—Ð°Ð²Ñ‚Ñ€Ð°Ðº",
                            "time": "08:00",
                            "calories": meal_calories,
                            "dishes": [{"name": "Ð—Ð°Ð²Ñ‚Ñ€Ð°Ðº Ð¿Ð¾ Ð¿Ð»Ð°Ð½Ñƒ", "calories": meal_calories}]
                        },
                        {
                            "name": "ÐžÐ±ÐµÐ´", 
                            "time": "13:00",
                            "calories": meal_calories,
                            "dishes": [{"name": "ÐžÐ±ÐµÐ´ Ð¿Ð¾ Ð¿Ð»Ð°Ð½Ñƒ", "calories": meal_calories}]
                        },
                        {
                            "name": "Ð£Ð¶Ð¸Ð½",
                            "time": "18:00", 
                            "calories": meal_calories,
                            "dishes": [{"name": "Ð£Ð¶Ð¸Ð½ Ð¿Ð¾ Ð¿Ð»Ð°Ð½Ñƒ", "calories": meal_calories}]
                        },
                        {
                            "name": "ÐŸÐµÑ€ÐµÐºÑƒÑ",
                            "time": "21:00",
                            "calories": meal_calories,
                            "dishes": [{"name": "ÐŸÐµÑ€ÐµÐºÑƒÑ Ð¿Ð¾ Ð¿Ð»Ð°Ð½Ñƒ", "calories": meal_calories}]
                        }
                    ]
                }
                for day in ["ÐŸÐ¾Ð½ÐµÐ´ÐµÐ»ÑŒÐ½Ð¸Ðº", "Ð’Ñ‚Ð¾Ñ€Ð½Ð¸Ðº", "Ð¡Ñ€ÐµÐ´Ð°", "Ð§ÐµÑ‚Ð²ÐµÑ€Ð³", "ÐŸÑÑ‚Ð½Ð¸Ñ†Ð°", "Ð¡ÑƒÐ±Ð±Ð¾Ñ‚Ð°", "Ð’Ð¾ÑÐºÑ€ÐµÑÐµÐ½ÑŒÐµ"]
            ]
        }

    async def generate_nutrition_plan(
        self,
        nutrition_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ Ð¿Ð»Ð°Ð½Ð° Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ
        
        Args:
            nutrition_data: Ð”Ð°Ð½Ð½Ñ‹Ðµ Ð¾ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ð¸ ÐºÐ»Ð¸ÐµÐ½Ñ‚Ð°
        
        Returns:
            Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ Ð¿Ð»Ð°Ð½ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ Ð² JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ
        """
        
        # Ð’Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ñ Ð²Ñ…Ð¾Ð´Ð½Ñ‹Ñ… Ð´Ð°Ð½Ð½Ñ‹Ñ…
        required_fields = ["nutrition_goal", "daily_calories"]
        for field in required_fields:
            if field not in nutrition_data:
                raise ValidationError(f"ÐžÑ‚ÑÑƒÑ‚ÑÑ‚Ð²ÑƒÐµÑ‚ Ð¾Ð±ÑÐ·Ð°Ñ‚ÐµÐ»ÑŒÐ½Ð¾Ðµ Ð¿Ð¾Ð»Ðµ: {field}")
        
        try:
            # Try LLM generation first
            preferences_str = ", ".join(nutrition_data.get("food_preferences", ["Ð¾Ð±Ñ‹Ñ‡Ð½Ð¾Ðµ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ðµ"]))
            allergies_str = ", ".join(nutrition_data.get("allergies", ["Ð½ÐµÑ‚"]))
            
            user_prompt = f"""
            Ð¡Ð¾Ð·Ð´Ð°Ð¹ Ð¿Ñ€Ð¾ÑÑ‚Ð¾Ð¹ Ð¿Ð»Ð°Ð½ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ:
            Ð¦ÐµÐ»ÑŒ: {nutrition_data["nutrition_goal"]}
            ÐšÐ°Ð»Ð¾Ñ€Ð¸Ð¸: {nutrition_data["daily_calories"]}
            
            Ð¤Ð¾Ñ€Ð¼Ð°Ñ‚ JSON:
            {{
                "goal": "{nutrition_data['nutrition_goal']}",
                "daily_calories": {nutrition_data["daily_calories"]},
                "daily_protein": 150,
                "daily_fats": 80,
                "daily_carbs": 300,
                "days": [
                    {{
                        "day_name": "ÐŸÐ¾Ð½ÐµÐ´ÐµÐ»ÑŒÐ½Ð¸Ðº",
                        "meals": [
                            {{
                                "name": "Ð—Ð°Ð²Ñ‚Ñ€Ð°Ðº",
                                "calories": 500,
                                "dishes": [{{
                                    "name": "ÐžÐ²ÑÑÐ½ÐºÐ°",
                                    "calories": 300
                                }}]
                            }}
                        ]
                    }}
                ]
            }}
            """
            
            messages = [
                {
                    "role": "system",
                    "content": "Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸ÑŽ. ÐžÑ‚Ð²ÐµÑ‡Ð°Ð¹ Ð¢ÐžÐ›Ð¬ÐšÐž Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¼ JSON."
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
            
            result = await self._make_openai_request(
                messages=messages,
                request_type=LLMRequestType.PROGRAM_CREATE,
                max_tokens=1500  # Reduced to encourage simpler response
            )
            
            try:
                nutrition_plan = json.loads(result["content"])
                
                return {
                    "plan": nutrition_plan,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "llm"
                    }
                }
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLM Ð¾Ñ‚Ð²ÐµÑ‚ Ð½Ðµ Ð²Ð°Ð»Ð¸Ð´Ð½Ñ‹Ð¹ JSON: {e}")
                logger.info("Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback Ð¿Ð»Ð°Ð½ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ")
                
                # Use fallback plan
                fallback_plan = self._create_fallback_nutrition_plan(nutrition_data)
                
                return {
                    "plan": fallback_plan,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "fallback"
                    }
                }
        
        except Exception as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð³ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ð¸ Ð¿Ð»Ð°Ð½Ð° Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ: {e}")
            logger.info("Ð˜ÑÐ¿Ð¾Ð»ÑŒÐ·ÑƒÐµÐ¼ fallback Ð¿Ð»Ð°Ð½ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ")
            
            # Create fallback plan
            fallback_plan = self._create_fallback_nutrition_plan(nutrition_data)
            
            return {
                "plan": fallback_plan,
                "metadata": {
                    "tokens_used": 0,
                    "model": "fallback",
                    "latency_ms": 0,
                    "source": "fallback"
                }
            }
    
    async def generate_shopping_list(
        self,
        nutrition_plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Ð“ÐµÐ½ÐµÑ€Ð°Ñ†Ð¸Ñ ÑÐ¿Ð¸ÑÐºÐ° Ð¿Ð¾ÐºÑƒÐ¿Ð¾Ðº Ð½Ð° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ð»Ð°Ð½Ð° Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ
        
        Args:
            nutrition_plan: ÐŸÐ»Ð°Ð½ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ
        
        Returns:
            Ð¡Ñ‚Ñ€ÑƒÐºÑ‚ÑƒÑ€Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ð¾ÐºÑƒÐ¿Ð¾Ðº
        """
        
        user_prompt = f"""
        ÐÐ° Ð¾ÑÐ½Ð¾Ð²Ðµ Ð¿Ð»Ð°Ð½Ð° Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ ÑÐ¾Ð·Ð´Ð°Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð¿Ð¾ÐºÑƒÐ¿Ð¾Ðº Ð½Ð° Ð½ÐµÐ´ÐµÐ»ÑŽ.
        
        ÐŸÐ»Ð°Ð½ Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ:
        {json.dumps(nutrition_plan, ensure_ascii=False, indent=2)}
        
        Ð¡Ð¾Ð·Ð´Ð°Ð¹ ÑÐ¿Ð¸ÑÐ¾Ðº Ð² JSON Ñ„Ð¾Ñ€Ð¼Ð°Ñ‚Ðµ:
        {{
            "shopping_list": {{
                "Ð¼Ð¾Ð»Ð¾Ñ‡Ð½Ñ‹Ðµ_Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹": [
                    {{"name": "ÐœÐ¾Ð»Ð¾ÐºÐ¾", "quantity": "2Ð»"}},
                    {{"name": "Ð¢Ð²Ð¾Ñ€Ð¾Ð³", "quantity": "500Ð³"}}
                ],
                "Ð¼ÑÑÐ¾_Ñ€Ñ‹Ð±Ð°": [
                    {{"name": "ÐšÑƒÑ€Ð¸Ð½Ð°Ñ Ð³Ñ€ÑƒÐ´ÐºÐ°", "quantity": "1ÐºÐ³"}}
                ],
                "Ð¾Ð²Ð¾Ñ‰Ð¸_Ñ„Ñ€ÑƒÐºÑ‚Ñ‹": [
                    {{"name": "ÐŸÐ¾Ð¼Ð¸Ð´Ð¾Ñ€Ñ‹", "quantity": "500Ð³"}}
                ],
                "ÐºÑ€ÑƒÐ¿Ñ‹_Ð±Ð¾Ð±Ð¾Ð²Ñ‹Ðµ": [
                    {{"name": "Ð“Ñ€ÐµÑ‡ÐºÐ°", "quantity": "500Ð³"}}
                ],
                "Ð¿Ñ€Ð¾Ñ‡ÐµÐµ": [
                    {{"name": "Ð¡Ð¾Ð»ÑŒ", "quantity": "1 Ð¿Ð°Ñ‡ÐºÐ°"}}
                ]
            }}
        }}
        
        Ð£Ñ‡Ð¸Ñ‚Ñ‹Ð²Ð°Ð¹:
        - ÐžÐ±ÑŠÐµÐ´Ð¸Ð½Ð¸ Ð¾Ð´Ð¸Ð½Ð°ÐºÐ¾Ð²Ñ‹Ðµ Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ñ‹
        - Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ð°Ð¹ ÐºÐ¾Ð»Ð¸Ñ‡ÐµÑÑ‚Ð²Ð¾ Ð½Ð° Ð½ÐµÐ´ÐµÐ»ÑŽ
        - Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐ¹ Ð¿Ð¾ ÐºÐ°Ñ‚ÐµÐ³Ð¾Ñ€Ð¸ÑÐ¼
        - Ð£ÐºÐ°Ð¶Ð¸ ÑƒÐ´Ð¾Ð±Ð½ÑƒÑŽ Ð´Ð»Ñ Ð¿Ð¾ÐºÑƒÐ¿ÐºÐ¸ ÑƒÐ¿Ð°ÐºÐ¾Ð²ÐºÑƒ
        """
        
        messages = [
            {
                "role": "system", 
                "content": "Ð¢Ñ‹ ÑÐºÑÐ¿ÐµÑ€Ñ‚ Ð¿Ð¾ Ð¿Ð»Ð°Ð½Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸ÑŽ Ð¿Ð¾ÐºÑƒÐ¿Ð¾Ðº Ð¿Ñ€Ð¾Ð´ÑƒÐºÑ‚Ð¾Ð² Ð¿Ð¸Ñ‚Ð°Ð½Ð¸Ñ."
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.CHAT,
            max_tokens=1000
        )
        
        try:
            shopping_data = json.loads(result["content"])
            
            return {
                "shopping_list": shopping_data["shopping_list"],
                "metadata": {
                    "tokens_used": result["usage"]["total_tokens"],
                    "model": result["model"],
                    "latency_ms": result["latency_ms"]
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"ÐžÑˆÐ¸Ð±ÐºÐ° Ð¿Ð°Ñ€ÑÐ¸Ð½Ð³Ð° ÑÐ¿Ð¸ÑÐºÐ° Ð¿Ð¾ÐºÑƒÐ¿Ð¾Ðº: {e}")
            raise LLMServiceError("ÐžÑˆÐ¸Ð±ÐºÐ° ÑÐ¾Ð·Ð´Ð°Ð½Ð¸Ñ ÑÐ¿Ð¸ÑÐºÐ° Ð¿Ð¾ÐºÑƒÐ¿Ð¾Ðº")

    async def get_completion(
        self,
        prompt: str,
        system_message: str = None,
        **kwargs
    ) -> str:
        """
        Simple method to get completion from LLM
        
        Args:
            prompt: User prompt
            system_message: Optional system message
            **kwargs: Additional parameters
        
        Returns:
            LLM response content
        """
        
        if not prompt.strip():
            raise ValidationError("ÐŸÑ€Ð¾Ð¼Ð¿Ñ‚ Ð½Ðµ Ð¼Ð¾Ð¶ÐµÑ‚ Ð±Ñ‹Ñ‚ÑŒ Ð¿ÑƒÑÑ‚Ñ‹Ð¼")
        
        # Ð¤Ð¾Ñ€Ð¼Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ðµ ÑÐ¾Ð¾Ð±Ñ‰ÐµÐ½Ð¸Ð¹
        messages = []
        
        if system_message:
            messages.append({
                "role": "system",
                "content": system_message
            })
        
        messages.append({
            "role": "user", 
            "content": prompt
        })
        
        # Ð’Ñ‹Ð¿Ð¾Ð»Ð½ÐµÐ½Ð¸Ðµ Ð·Ð°Ð¿Ñ€Ð¾ÑÐ°
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.CHAT,
            **kwargs
        )
        
        return result["content"].strip()


# Ð“Ð»Ð¾Ð±Ð°Ð»ÑŒÐ½Ñ‹Ð¹ ÑÐºÐ·ÐµÐ¼Ð¿Ð»ÑÑ€ ÑÐµÑ€Ð²Ð¸ÑÐ°
llm_service = LLMService() 