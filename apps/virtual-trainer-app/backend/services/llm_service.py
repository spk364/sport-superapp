"""
LLM —Å–µ—Ä–≤–∏—Å –¥–ª—è Virtual Trainer
–ò–Ω—Ç–µ–≥—Ä–∞—Ü–∏—è —Å OpenAI API –∏ –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤—Å–µ—Ö —Ç–∏–ø–æ–≤ LLM –∑–∞–ø—Ä–æ—Å–æ–≤
"""

import json
import time
from typing import Dict, List, Optional, Any, Union
from datetime import datetime
import asyncio
from loguru import logger
from openai import AsyncOpenAI

from backend.core.config import settings
from backend.core.exceptions import LLMServiceError, ValidationError
from backend.services.rag_tools_service import rag_tools
from backend.services.knowledge_base_service import knowledge_base
from backend.services.llm_service_rag_methods import LLMServiceRAGMethods
from backend.database.models import LLMRequestType


class LLMService:
    """–°–µ—Ä–≤–∏—Å –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å —è–∑—ã–∫–æ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏"""
    
    def __init__(self):
        # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ OpenAI –∫–ª–∏–µ–Ω—Ç–∞
        self.client = AsyncOpenAI(api_key=settings.OPENAI_API_KEY)
        self.model = settings.OPENAI_MODEL
        self.temperature = settings.OPENAI_TEMPERATURE
        self.max_tokens = settings.OPENAI_MAX_TOKENS
        self.timeout = settings.OPENAI_TIMEOUT
        
        # –°–∏—Å—Ç–µ–º–Ω—ã–µ –ø—Ä–æ–º–ø—Ç—ã
        self.system_prompts = settings.SYSTEM_PROMPTS
        
        # –°–µ–º–∞—Ñ–æ—Ä –¥–ª—è –æ–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è –æ–¥–Ω–æ–≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –∑–∞–ø—Ä–æ—Å–æ–≤
        self.semaphore = asyncio.Semaphore(settings.MAX_CONCURRENT_LLM_REQUESTS)
    
    async def _make_openai_request(
        self,
        messages: List[Dict[str, str]],
        request_type: LLMRequestType,
        **kwargs
    ) -> Dict[str, Any]:
        """–ë–∞–∑–æ–≤—ã–π –º–µ—Ç–æ–¥ –¥–ª—è –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ OpenAI"""
        
        async with self.semaphore:
            start_time = time.time()
            
            try:
                # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤
                params = {
                    "model": kwargs.get("model", self.model),
                    "messages": messages,
                    "temperature": kwargs.get("temperature", self.temperature),
                    "max_tokens": kwargs.get("max_tokens", self.max_tokens),
                }
                
                # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
                response = await self.client.chat.completions.create(**params)
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
                result = {
                    "content": response.choices[0].message.content,
                    "usage": {
                        "prompt_tokens": response.usage.prompt_tokens,
                        "completion_tokens": response.usage.completion_tokens,
                        "total_tokens": response.usage.total_tokens,
                    },
                    "model": response.model,
                    "latency_ms": int((time.time() - start_time) * 1000)
                }
                
                logger.info(f"LLM –∑–∞–ø—Ä–æ—Å {request_type.value} –≤—ã–ø–æ–ª–Ω–µ–Ω —É—Å–ø–µ—à–Ω–æ")
                return result
                
            except Exception as e:
                error_msg = str(e)
                logger.error(f"–û—à–∏–±–∫–∞ LLM: {error_msg}")
                
                # –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö —Ç–∏–ø–æ–≤ –æ—à–∏–±–æ–∫
                if "rate_limit" in error_msg.lower() or "429" in error_msg:
                    raise LLMServiceError("–ü—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç –∑–∞–ø—Ä–æ—Å–æ–≤ –∫ –ò–ò. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –ø–æ–∑–∂–µ.")
                elif "auth" in error_msg.lower() or "401" in error_msg:
                    raise LLMServiceError("–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ —Å–ª—É–∂–±–µ –ò–ò")
                elif "api" in error_msg.lower() or "500" in error_msg:
                    raise LLMServiceError("–í—Ä–µ–º–µ–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–ª—É–∂–±—ã –ò–ò")
                else:
                    raise LLMServiceError(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω–∞—è –æ—à–∏–±–∫–∞ —Å–ª—É–∂–±—ã –ò–ò: {error_msg}")
    
    async def chat_with_virtual_trainer(
        self,
        user_message: str,
        chat_history: List[Dict[str, str]] = None,
        user_context: Optional[Dict[str, Any]] = None,
        user_id: str = None,
        session_id: str = None
    ) -> Dict[str, Any]:
        """
        –ß–∞—Ç —Å –≤–∏—Ä—Ç—É–∞–ª—å–Ω—ã–º —Ç—Ä–µ–Ω–µ—Ä–æ–º
        
        Args:
            user_message: –°–æ–æ–±—â–µ–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
            chat_history: –ò—Å—Ç–æ—Ä–∏—è —á–∞—Ç–∞ (–ø–æ—Å–ª–µ–¥–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏—è)
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è (—Ü–µ–ª–∏, —É—Ä–æ–≤–µ–Ω—å –∏ —Ç.–¥.)
        
        Returns:
            –û—Ç–≤–µ—Ç –≤–∏—Ä—Ç—É–∞–ª—å–Ω–æ–≥–æ —Ç—Ä–µ–Ω–µ—Ä–∞
        """
        
        if not user_message.strip():
            raise ValidationError("–°–æ–æ–±—â–µ–Ω–∏–µ –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        context_info = ""
        if user_context:
            # –§–∏–∑–∏—á–µ—Å–∫–∏–µ —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∏—Å—Ç–∏–∫–∏
            physical_info = []
            if user_context.get("age"):
                physical_info.append(f"–≤–æ–∑—Ä–∞—Å—Ç {user_context['age']} –ª–µ—Ç")
            if user_context.get("gender"):
                physical_info.append(f"–ø–æ–ª {user_context['gender']}")
            if user_context.get("height"):
                physical_info.append(f"—Ä–æ—Å—Ç {user_context['height']}")
            if user_context.get("weight"):
                physical_info.append(f"–≤–µ—Å {user_context['weight']}")
            if physical_info:
                context_info += f"–§–∏–∑–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ: {', '.join(physical_info)}. "
            
            # –§–∏—Ç–Ω–µ—Å-–∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è
            if user_context.get("goals"):
                if isinstance(user_context['goals'], list):
                    goals_str = ', '.join(user_context['goals'])
                else:
                    goals_str = user_context['goals']
                context_info += f"–¶–µ–ª–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫: {goals_str}. "
            
            if user_context.get("fitness_level"):
                context_info += f"–£—Ä–æ–≤–µ–Ω—å –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏: {user_context['fitness_level']}. "
            
            if user_context.get("equipment"):
                if isinstance(user_context['equipment'], list):
                    equipment_str = ', '.join(user_context['equipment'])
                else:
                    equipment_str = user_context['equipment']
                context_info += f"–î–æ—Å—Ç—É–ø–Ω–æ–µ –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: {equipment_str}. "
            
            if user_context.get("limitations"):
                if isinstance(user_context['limitations'], list):
                    limitations_str = ', '.join(user_context['limitations'])
                else:
                    limitations_str = user_context['limitations']
                context_info += f"–û–≥—Ä–∞–Ω–∏—á–µ–Ω–∏—è: {limitations_str}. "
            
            # –ü–∏—Ç–∞–Ω–∏–µ
            nutrition_info = []
            if user_context.get("nutrition_goal"):
                nutrition_info.append(f"—Ü–µ–ª—å –ø–∏—Ç–∞–Ω–∏—è: {user_context['nutrition_goal']}")
            if user_context.get("food_preferences"):
                if isinstance(user_context['food_preferences'], list):
                    prefs_str = ', '.join(user_context['food_preferences'])
                else:
                    prefs_str = user_context['food_preferences']
                nutrition_info.append(f"–ø—Ä–µ–¥–ø–æ—á—Ç–µ–Ω–∏—è: {prefs_str}")
            if user_context.get("allergies"):
                if isinstance(user_context['allergies'], list):
                    allergies_str = ', '.join(user_context['allergies'])
                else:
                    allergies_str = user_context['allergies']
                nutrition_info.append(f"–∞–ª–ª–µ—Ä–≥–∏–∏: {allergies_str}")
            if nutrition_info:
                context_info += f"–ü–∏—Ç–∞–Ω–∏–µ: {', '.join(nutrition_info)}. "
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["virtual_trainer"] + 
                          (f"\n\n–ö–æ–Ω—Ç–µ–∫—Å—Ç –∫–ª–∏–µ–Ω—Ç–∞: {context_info}" if context_info else "")
            }
        ]
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ –∏—Å—Ç–æ—Ä–∏–∏ —á–∞—Ç–∞
        if chat_history:
            for msg in chat_history[-settings.MAX_CHAT_HISTORY:]:
                messages.append({
                    "role": msg["role"],
                    "content": msg["content"]
                })
        
        # –î–æ–±–∞–≤–ª–µ–Ω–∏–µ —Ç–µ–∫—É—â–µ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        messages.append({
            "role": "user",
            "content": user_message
        })
        
        # Store conversation in knowledge base
        if user_id and session_id:
            try:
                # Store user message
                await knowledge_base.store_conversation_message(
                    user_id=user_id,
                    session_id=session_id,
                    role="user",
                    content=user_message,
                    importance_score=1.0
                )
            except Exception as e:
                logger.warning(f"Failed to store user message in knowledge base: {e}")

        # Universal approach: Always provide RAG tools, let LLM decide when to use them
        if user_id and session_id:
            logger.info("Using RAG-enhanced chat with LLM decision-making")
            
            # Always use RAG tools, let AI decide when to call them
            final_result = await self._chat_with_rag_tools(
                messages=messages,
                user_id=user_id,
                session_id=session_id,
                user_message=user_message
            )
            
            if not final_result:
                # Fallback to normal chat if RAG fails
                logger.warning("RAG-enhanced chat failed, falling back to normal chat")
                final_result = await self._make_openai_request(
                    messages=messages,
                    request_type=LLMRequestType.CHAT
                )
                final_result["used_rag"] = False
        else:
            # Normal chat without RAG (no user context)
            logger.info("Using normal chat (no user context)")
            final_result = await self._make_openai_request(
                messages=messages,
                request_type=LLMRequestType.CHAT
            )
            final_result["used_rag"] = False

        # Store AI response in knowledge base
        if user_id and session_id:
            try:
                await knowledge_base.store_conversation_message(
                    user_id=user_id,
                    session_id=session_id,
                    role="assistant",
                    content=final_result["content"],
                    importance_score=1.2 if final_result.get("used_rag", False) else 1.0
                )
            except Exception as e:
                logger.warning(f"Failed to store AI message in knowledge base: {e}")

        return {
            "response": final_result["content"],
            "used_rag": final_result.get("used_rag", False),
            "metadata": {
                "tokens_used": final_result["usage"]["total_tokens"],
                "model": final_result["model"],
                "latency_ms": final_result["latency_ms"]
            }
        }
    
    async def _should_use_rag_tools(self, user_message: str, chat_history: List[Dict[str, str]] = None) -> bool:
        """
        DEPRECATED: This function is no longer used.
        RAG tools are now always available and the LLM decides when to use them.
        """
        # This function is kept for backward compatibility but not used
        return True
    
    async def _chat_with_rag_tools(
        self,
        messages: List[Dict[str, str]],
        user_id: str,
        session_id: str,
        user_message: str
    ) -> Dict[str, Any]:
        """
        Enhanced chat that provides RAG tools for AI to use when needed
        
        Args:
            messages: Chat messages to send
            user_id: User ID for RAG filtering
            session_id: Session ID for RAG filtering  
            user_message: Current user message
            
        Returns:
            OpenAI response with RAG enhancement tracking
        """
        
        try:
            # Add RAG tools to the request
            tools = rag_tools.get_tool_definitions()
            
            # Enhanced system message for intelligent RAG usage
            rag_system_message = """
            
            üß† –ò–ù–°–¢–†–£–ú–ï–ù–¢–´ –ü–ê–ú–Ø–¢–ò:
            –£ —Ç–µ–±—è –µ—Å—Ç—å –¥–æ—Å—Ç—É–ø –∫ –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ —Ä–∞–∑–≥–æ–≤–æ—Ä–æ–≤ —Å —ç—Ç–∏–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º. –ò—Å–ø–æ–ª—å–∑—É–π —ç—Ç–∏ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã —Ä–∞–∑—É–º–Ω–æ:
            
            üìã search_conversation_history - –Ω–∞–π—Ç–∏ –∫–æ–Ω–∫—Ä–µ—Ç–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ –ø—Ä–æ—à–ª—ã—Ö –±–µ—Å–µ–¥
            üìä get_conversation_summary - –ø–æ–ª—É—á–∏—Ç—å –æ–±–∑–æ—Ä –Ω–µ–¥–∞–≤–Ω–∏—Ö —Ç–µ–º –∏ –æ–±—Å—É–∂–¥–µ–Ω–∏–π  
            üîç find_related_discussions - –Ω–∞–π—Ç–∏ –≤—Å–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ –æ–±—Å—É–∂–¥–µ–Ω–∏—è –ø–æ —Ç–µ–º–µ
            
            –ö–û–ì–î–ê –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨:
            ‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å—Å—ã–ª–∞–µ—Ç—Å—è –Ω–∞ –ø—Ä–æ—à–ª—ã–µ —Ä–∞–∑–≥–æ–≤–æ—Ä—ã ("–ø–æ–º–Ω–∏—à—å", "–º—ã –æ–±—Å—É–∂–¥–∞–ª–∏", "—Ç—ã –≥–æ–≤–æ—Ä–∏–ª")
            ‚úÖ –°–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ —Å–≤–æ–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–µ, –ø—Ä–æ–≥—Ä–µ—Å—Å–µ, —Ü–µ–ª—è—Ö ("–º–æ—è –ø—Ä–æ–≥—Ä–∞–º–º–∞", "–∫–∞–∫ –¥–µ–ª–∞ —Å...")
            ‚úÖ –í–æ–ø—Ä–æ—Å—ã —Ç—Ä–µ–±—É—é—Ç –ø–µ—Ä—Å–æ–Ω–∞–ª—å–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞ ("–∫–æ–≥–¥–∞ —è...", "—á—Ç–æ —è...")
            ‚úÖ –ù—É–∂–Ω–∞ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è—Ö –∏–ª–∏ –ø–ª–∞–Ω–∞—Ö
            ‚úÖ –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å —Å–ø—Ä–∞—à–∏–≤–∞–µ—Ç –æ –≤—Ä–µ–º–µ–Ω–∏/–¥–∞—Ç–∞—Ö —Å–æ–±—ã—Ç–∏–π ("–∫–æ–≥–¥–∞", "–≤—á–µ—Ä–∞", "–Ω–∞ –ø—Ä–æ—à–ª–æ–π –Ω–µ–¥–µ–ª–µ")
            
            –ö–û–ì–î–ê –ù–ï –ò–°–ü–û–õ–¨–ó–û–í–ê–¢–¨:
            ‚ùå –û–±—â–∏–µ –≤–æ–ø—Ä–æ—Å—ã –æ —Ñ–∏—Ç–Ω–µ—Å–µ/–ø–∏—Ç–∞–Ω–∏–∏ –±–µ–∑ –ª–∏—á–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            ‚ùå –ù–æ–≤—ã–µ —Ç–µ–º—ã, –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∏—Å—Ç–æ—Ä–∏–µ–π
            ‚ùå –ü—Ä–æ—Å—Ç—ã–µ –ø—Ä–∏–≤–µ—Ç—Å—Ç–≤–∏—è –∏–ª–∏ –±–ª–∞–≥–æ–¥–∞—Ä–Ω–æ—Å—Ç–∏
            ‚ùå –í–æ–ø—Ä–æ—Å—ã, –Ω–∞ –∫–æ—Ç–æ—Ä—ã–µ –º–æ–∂–µ—à—å –æ—Ç–≤–µ—Ç–∏—Ç—å –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ç–µ–∫—É—â–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
            
            –ü—Ä–∏–Ω–∏–º–∞–π —Ä–µ—à–µ–Ω–∏–µ —Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ - –µ—Å–ª–∏ —Å—á–∏—Ç–∞–µ—à—å —á—Ç–æ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –∏–∑ –∏—Å—Ç–æ—Ä–∏–∏ –ø–æ–º–æ–∂–µ—Ç –¥–∞—Ç—å –ª—É—á—à–∏–π –æ—Ç–≤–µ—Ç, –∏—Å–ø–æ–ª—å–∑—É–π –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç—ã.
            """
            
            # Modify the first system message to include RAG instructions
            if messages and messages[0]["role"] == "system":
                messages[0]["content"] = messages[0]["content"] + rag_system_message
            
            # Make request with tools
            response = await self._make_openai_request_with_tools(
                messages=messages,
                tools=tools,
                user_id=user_id,
                session_id=session_id
            )
            
            return response
            
        except Exception as e:
            logger.error(f"Error in RAG-enhanced chat: {e}")
            return None
    
    async def _make_openai_request_with_tools(
        self,
        messages: List[Dict[str, str]],
        tools: List[Dict[str, Any]],
        user_id: str,
        session_id: str
    ) -> Dict[str, Any]:
        """
        Make OpenAI request with function calling tools
        
        Args:
            messages: Chat messages
            tools: Available tools for function calling
            user_id: User ID for tool execution
            session_id: Session ID for tool execution
            
        Returns:
            Final response after tool execution
        """
        
        start_time = time.time()
        total_tokens = 0
        
        try:
            # Initial request with tools
            response = await self.client.chat.completions.create(
                model=settings.OPENAI_MODEL,
                messages=messages,
                tools=tools,
                tool_choice="auto",
                temperature=settings.OPENAI_TEMPERATURE,
                max_tokens=settings.OPENAI_MAX_TOKENS,
                timeout=settings.OPENAI_TIMEOUT
            )
            
            total_tokens += response.usage.total_tokens
            
            # Check if AI wants to use tools
            if response.choices[0].message.tool_calls:
                logger.info(f"AI requested {len(response.choices[0].message.tool_calls)} tool calls")
                
                # Add AI message with tool calls to conversation
                messages.append({
                    "role": "assistant",
                    "content": response.choices[0].message.content,
                    "tool_calls": [
                        {
                            "id": tool_call.id,
                            "type": tool_call.type,
                            "function": {
                                "name": tool_call.function.name,
                                "arguments": tool_call.function.arguments
                            }
                        }
                        for tool_call in response.choices[0].message.tool_calls
                    ]
                })
                
                # Execute each tool call
                for tool_call in response.choices[0].message.tool_calls:
                    try:
                        function_name = tool_call.function.name
                        function_args = json.loads(tool_call.function.arguments)
                        
                        logger.info(f"Executing tool: {function_name} with args: {function_args}")
                        
                        # Execute the tool
                        tool_result = await rag_tools.execute_tool(
                            tool_name=function_name,
                            tool_arguments=function_args,
                            user_id=user_id,
                            session_id=session_id
                        )
                        
                        # Add tool result to conversation
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps(tool_result, ensure_ascii=False)
                        })
                        
                    except Exception as e:
                        logger.error(f"Error executing tool {tool_call.function.name}: {e}")
                        # Add error message
                        messages.append({
                            "role": "tool",
                            "tool_call_id": tool_call.id,
                            "content": json.dumps({"error": str(e)}, ensure_ascii=False)
                        })
                
                # Get final response after tool execution
                final_response = await self.client.chat.completions.create(
                    model=settings.OPENAI_MODEL,
                    messages=messages,
                    temperature=settings.OPENAI_TEMPERATURE,
                    max_tokens=settings.OPENAI_MAX_TOKENS,
                    timeout=settings.OPENAI_TIMEOUT
                )
                
                total_tokens += final_response.usage.total_tokens
                
                end_time = time.time()
                latency_ms = int((end_time - start_time) * 1000)
                
                return {
                    "content": final_response.choices[0].message.content,
                    "usage": {"total_tokens": total_tokens},
                    "model": final_response.model,
                    "latency_ms": latency_ms,
                    "used_rag": True
                }
            
            else:
                # No tools used, return direct response
                end_time = time.time()
                latency_ms = int((end_time - start_time) * 1000)
                
                return {
                    "content": response.choices[0].message.content,
                    "usage": {"total_tokens": total_tokens},
                    "model": response.model,
                    "latency_ms": latency_ms,
                    "used_rag": False
                }
                
        except Exception as e:
            logger.error(f"Error in OpenAI request with tools: {e}")
            raise LLMServiceError(f"Tool-enhanced chat failed: {str(e)}")
    
    def _create_fallback_workout_program(self, client_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a basic fallback workout program if LLM fails"""
        goal = client_data.get("goal", "–æ–±—â–∞—è —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∞")
        level = client_data.get("level", "–Ω–∞—á–∞–ª—å–Ω—ã–π")
        equipment = client_data.get("equipment", ["—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–µ—Å"])
        
        # Basic program structure
        basic_exercises = {
            "upper_body": ["–û—Ç–∂–∏–º–∞–Ω–∏—è", "–ü–ª–∞–Ω–∫–∞", "–ü–æ–¥—Ç—è–≥–∏–≤–∞–Ω–∏—è"],
            "lower_body": ["–ü—Ä–∏—Å–µ–¥–∞–Ω–∏—è", "–í—ã–ø–∞–¥—ã", "–ü–æ–¥—ä–µ–º—ã –Ω–∞ –Ω–æ—Å–∫–∏"],
            "cardio": ["–ë–µ–≥ –Ω–∞ –º–µ—Å—Ç–µ", "–ü—Ä—ã–∂–∫–∏", "–ë–µ—Ä–ø–∏"]
        }
        
        # Adjust difficulty based on level
        if "–Ω–∞—á–∞–ª—å–Ω—ã–π" in level.lower():
            sets, reps = 2, "8-10"
        elif "—Å—Ä–µ–¥–Ω–∏–π" in level.lower():
            sets, reps = 3, "10-12"
        else:  # advanced
            sets, reps = 4, "12-15"
        
        weeks = []
        for week_num in range(1, 5):  # 4 weeks
            workouts = []
            
            # 3 workouts per week
            workout_types = ["–í–µ—Ä—Ö —Ç–µ–ª–∞", "–ù–∏–∑ —Ç–µ–ª–∞", "–ö–∞—Ä–¥–∏–æ"]
            exercise_groups = ["upper_body", "lower_body", "cardio"]
            
            for i, (workout_type, exercise_group) in enumerate(zip(workout_types, exercise_groups)):
                exercises = []
                for exercise_name in basic_exercises[exercise_group]:
                    exercises.append({
                        "name": exercise_name,
                        "sets": sets,
                        "reps": reps,
                        "weight": 0,
                        "notes": ""
                    })
                
                workouts.append({
                    "name": workout_type,
                    "exercises": exercises
                })
            
            weeks.append({
                "week_number": week_num,
                "workouts": workouts
            })
        
        return {
            "goal": goal,
            "level": level,
            "duration_weeks": 4,
            "workouts_per_week": 3,
            "equipment": equipment,
            "weeks": weeks
        }

    async def generate_workout_program(
        self,
        client_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã
        
        Args:
            client_data: –î–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞ (—Ü–µ–ª–∏, —É—Ä–æ–≤–µ–Ω—å, –æ–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ –∏ —Ç.–¥.)
        
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
        """
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        required_fields = ["goal", "level", "sessions_per_week"]
        for field in required_fields:
            if field not in client_data:
                raise ValidationError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
        
        try:
            # Try LLM generation first
            equipment_str = ", ".join(client_data.get("equipment", ["—Å–æ–±—Å—Ç–≤–µ–Ω–Ω—ã–π –≤–µ—Å"]))
            limitations_str = ", ".join(client_data.get("limitations", ["–Ω–µ—Ç"]))
            
            user_prompt = f"""
            –°–æ–∑–¥–∞–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –Ω–∞ 4 –Ω–µ–¥–µ–ª–∏:
            –¶–µ–ª—å: {client_data["goal"]}
            –£—Ä–æ–≤–µ–Ω—å: {client_data["level"]}
            –û–±–æ—Ä—É–¥–æ–≤–∞–Ω–∏–µ: {equipment_str}
            
            JSON —Ñ–æ—Ä–º–∞—Ç:
            {{
                "goal": "{client_data['goal']}",
                "level": "{client_data['level']}",
                "duration_weeks": 4,
                "workouts_per_week": 3,
                "equipment": ["{equipment_str}"],
                "weeks": [
                    {{
                        "week_number": 1,
                        "workouts": [
                            {{
                                "name": "–¢—Ä–µ–Ω–∏—Ä–æ–≤–∫–∞ 1",
                                "exercises": [
                                    {{"name": "–ü—Ä–∏—Å–µ–¥–∞–Ω–∏—è", "sets": 3, "reps": "10-12", "weight": 0}}
                                ]
                            }}
                        ]
                    }}
                ]
            }}
            """
            
            messages = [
                {
                    "role": "system", 
                    "content": "–¢—ã —Ñ–∏—Ç–Ω–µ—Å-—Ç—Ä–µ–Ω–µ—Ä. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
            
            # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
            result = await self._make_openai_request(
                messages=messages,
                request_type=LLMRequestType.PROGRAM_CREATE,
                max_tokens=1500  # Reduced for simpler response
            )
            
            # –ü–∞—Ä—Å–∏–Ω–≥ JSON
            try:
                # Clean response
                cleaned_content = self._clean_json_response(result["content"])
                program_data = json.loads(cleaned_content)
                
                # –í–∞–ª–∏–¥–∞—Ü–∏—è —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
                if "weeks" not in program_data:
                    raise ValueError("–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∞ weeks")
                
                return {
                    "program": program_data,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "llm"
                    }
                }
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLM –æ—Ç–≤–µ—Ç –Ω–µ –≤–∞–ª–∏–¥–Ω—ã–π JSON: {e}")
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—Ä–æ–≥—Ä–∞–º–º—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫")
                
                # Use fallback program
                fallback_program = self._create_fallback_workout_program(client_data)
                
                return {
                    "program": fallback_program,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "fallback"
                    }
                }
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫: {e}")
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø—Ä–æ–≥—Ä–∞–º–º—É —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ–∫")
            
            # Create fallback program
            fallback_program = self._create_fallback_workout_program(client_data)
            
            return {
                "program": fallback_program,
                "metadata": {
                    "tokens_used": 0,
                    "model": "fallback",
                    "latency_ms": 0,
                    "source": "fallback"
                }
            }
    
    async def adjust_workout_program(
        self,
        current_program: Dict[str, Any],
        feedback: str,
        progress_data: Optional[Dict[str, Any]] = None
    ) -> Dict[str, Any]:
        """
        –ö–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã
        
        Args:
            current_program: –¢–µ–∫—É—â–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞
            feedback: –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –æ—Ç –∫–ª–∏–µ–Ω—Ç–∞
            progress_data: –î–∞–Ω–Ω—ã–µ –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ
        
        Returns:
            –û–±–Ω–æ–≤–ª–µ–Ω–Ω–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞
        """
        
        if not feedback.strip():
            raise ValidationError("–û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç–æ–π")
        
        progress_info = ""
        if progress_data:
            progress_info = f"\n–î–∞–Ω–Ω—ã–µ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {json.dumps(progress_data, ensure_ascii=False)}"
        
        user_prompt = f"""
        –°–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä—É–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—É—é –ø—Ä–æ–≥—Ä–∞–º–º—É –Ω–∞ –æ—Å–Ω–æ–≤–µ –æ–±—Ä–∞—Ç–Ω–æ–π —Å–≤—è–∑–∏.
        
        –¢–µ–∫—É—â–∞—è –ø—Ä–æ–≥—Ä–∞–º–º–∞:
        {json.dumps(current_program, ensure_ascii=False, indent=2)}
        
        –û–±—Ä–∞—Ç–Ω–∞—è —Å–≤—è–∑—å –∫–ª–∏–µ–Ω—Ç–∞: {feedback}
        {progress_info}
        
        –í–µ—Ä–Ω–∏ –û–ë–ù–û–í–õ–ï–ù–ù–£–Æ –ø—Ä–æ–≥—Ä–∞–º–º—É –≤ —Ç–æ–º –∂–µ JSON —Ñ–æ—Ä–º–∞—Ç–µ, –Ω–æ —Å –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∞–º–∏.
        –°–æ—Ö—Ä–∞–Ω–∏ –æ–±—â—É—é —Å—Ç—Ä—É–∫—Ç—É—Ä—É, –Ω–æ –∞–¥–∞–ø—Ç–∏—Ä—É–π –Ω–∞–≥—Ä—É–∑–∫—É, —É–ø—Ä–∞–∂–Ω–µ–Ω–∏—è –∏–ª–∏ –ø–æ–¥—Ö–æ–¥—ã.
        """
        
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["program_generator"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.PROGRAM_ADJUST,
            max_tokens=2000
        )
        
        try:
            adjusted_program = json.loads(result["content"])
            
            return {
                "program": adjusted_program,
                "metadata": {
                    "tokens_used": result["usage"]["total_tokens"],
                    "model": result["model"],
                    "latency_ms": result["latency_ms"]
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–π –ø—Ä–æ–≥—Ä–∞–º–º—ã: {e}")
            raise LLMServiceError("–û—à–∏–±–∫–∞ –∫–æ—Ä—Ä–µ–∫—Ç–∏—Ä–æ–≤–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º—ã")
    
    async def analyze_progress(
        self,
        client_data: Dict[str, Any],
        period_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞ –∫–ª–∏–µ–Ω—Ç–∞
        
        Args:
            client_data: –î–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞
            period_data: –î–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥ (—Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–∏, –∑–∞–º–µ—Ä—ã)
        
        Returns:
            –û—Ç—á–µ—Ç —Å –∞–Ω–∞–ª–∏–∑–æ–º –∏ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è–º–∏
        """
        
        user_prompt = f"""
        –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π –ø—Ä–æ–≥—Ä–µ—Å—Å –∫–ª–∏–µ–Ω—Ç–∞ –∏ –¥–∞–π —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏.
        
        –î–∞–Ω–Ω—ã–µ –∫–ª–∏–µ–Ω—Ç–∞:
        {json.dumps(client_data, ensure_ascii=False, indent=2)}
        
        –î–∞–Ω–Ω—ã–µ –∑–∞ –ø–µ—Ä–∏–æ–¥:
        {json.dumps(period_data, ensure_ascii=False, indent=2)}
        
        –í–µ—Ä–Ω–∏ –∞–Ω–∞–ª–∏–∑ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
        {{
            "summary": "–ö—Ä–∞—Ç–∫–∏–π –∏—Ç–æ–≥ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞",
            "bottlenecks": ["–ø—Ä–æ–±–ª–µ–º–∞ 1", "–ø—Ä–æ–±–ª–µ–º–∞ 2"],
            "recommendations": ["—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 1", "—Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è 2"],
            "achievements": ["–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ 1", "–¥–æ—Å—Ç–∏–∂–µ–Ω–∏–µ 2"],
            "next_goals": ["—Ü–µ–ª—å 1", "—Ü–µ–ª—å 2"]
        }}
        """
        
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["progress_analyzer"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.PROGRESS_ANALYZE
        )
        
        try:
            analysis = json.loads(result["content"])
            
            return {
                "analysis": analysis,
                "metadata": {
                    "tokens_used": result["usage"]["total_tokens"],
                    "model": result["model"],
                    "latency_ms": result["latency_ms"]
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞: {e}")
            raise LLMServiceError("–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ –ø—Ä–æ–≥—Ä–µ—Å—Å–∞")
    
    async def generate_notification(
        self,
        notification_type: str,
        user_context: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
        
        Args:
            notification_type: –¢–∏–ø —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è
            user_context: –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
        
        Returns:
            –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ
        """
        
        type_prompts = {
            "workout_reminder": "–°–æ–∑–¥–∞–π –º–æ—Ç–∏–≤–∏—Ä—É—é—â–µ–µ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ –ø—Ä–µ–¥—Å—Ç–æ—è—â–µ–π —Ç—Ä–µ–Ω–∏—Ä–æ–≤–∫–µ",
            "payment_reminder": "–°–æ–∑–¥–∞–π –≤–µ–∂–ª–∏–≤–æ–µ –Ω–∞–ø–æ–º–∏–Ω–∞–Ω–∏–µ –æ–± –æ–ø–ª–∞—Ç–µ –∞–±–æ–Ω–µ–º–µ–Ω—Ç–∞",
            "motivational": "–°–æ–∑–¥–∞–π –º–æ—Ç–∏–≤–∞—Ü–∏–æ–Ω–Ω–æ–µ —Å–æ–æ–±—â–µ–Ω–∏–µ –¥–ª—è –∫–ª–∏–µ–Ω—Ç–∞",
            "progress_report": "–°–æ–∑–¥–∞–π —Å–æ–æ–±—â–µ–Ω–∏–µ —Å –ø—Ä–∏–≥–ª–∞—à–µ–Ω–∏–µ–º –ø–æ—Å–º–æ—Ç—Ä–µ—Ç—å –æ—Ç—á–µ—Ç –æ –ø—Ä–æ–≥—Ä–µ—Å—Å–µ"
        }
        
        specific_prompt = type_prompts.get(notification_type, "–°–æ–∑–¥–∞–π –ø–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–æ–µ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏–µ")
        
        user_prompt = f"""
        {specific_prompt}.
        
        –ö–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è:
        {json.dumps(user_context, ensure_ascii=False, indent=2)}
        
        –°–æ–æ–±—â–µ–Ω–∏–µ –¥–æ–ª–∂–Ω–æ –±—ã—Ç—å:
        - –ü–µ—Ä—Å–æ–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º
        - –î—Ä—É–∂–µ–ª—é–±–Ω—ã–º –Ω–æ –Ω–µ –Ω–∞–≤—è–∑—á–∏–≤—ã–º
        - –ú–æ—Ç–∏–≤–∏—Ä—É—é—â–∏–º
        - –ù–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
        - –î–ª–∏–Ω–æ–π –Ω–µ –±–æ–ª–µ–µ 200 —Å–∏–º–≤–æ–ª–æ–≤
        
        –í–µ—Ä–Ω–∏ —Ç–æ–ª—å–∫–æ —Ç–µ–∫—Å—Ç —Å–æ–æ–±—â–µ–Ω–∏—è –±–µ–∑ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–≥–æ —Ñ–æ—Ä–º–∞—Ç–∏—Ä–æ–≤–∞–Ω–∏—è.
        """
        
        messages = [
            {
                "role": "system",
                "content": self.system_prompts["notification_creator"]
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.NOTIFICATION_CREATE,
            max_tokens=100
        )
        
        return {
            "message": result["content"].strip(),
            "metadata": {
                "tokens_used": result["usage"]["total_tokens"],
                "model": result["model"],
                "latency_ms": result["latency_ms"]
            }
        }

    def _clean_json_response(self, content: str) -> str:
        """Clean and fix common JSON issues in LLM responses"""
        content = content.strip()
        
        # Remove markdown code blocks if present
        if content.startswith('```json'):
            content = content[7:]
        if content.startswith('```'):
            content = content[3:]
        if content.endswith('```'):
            content = content[:-3]
        
        # Find JSON block
        json_start = content.find('{')
        json_end = content.rfind('}')
        
        if json_start >= 0 and json_end > json_start:
            content = content[json_start:json_end+1]
        
        # Fix common issues
        content = content.replace("'", '"')  # Single quotes to double quotes
        content = content.replace(',,', ',')  # Remove double commas
        content = content.replace(',}', '}')  # Remove trailing commas before }
        content = content.replace(',]', ']')  # Remove trailing commas before ]
        
        return content

    def _create_fallback_nutrition_plan(self, nutrition_data: Dict[str, Any]) -> Dict[str, Any]:
        """Create a basic fallback nutrition plan if LLM fails"""
        daily_calories = nutrition_data["daily_calories"]
        goal = nutrition_data["nutrition_goal"]
        
        # Calculate macros
        if "–ø–æ—Ö—É–¥–µ–Ω–∏–µ" in goal.lower():
            protein_ratio, fat_ratio, carb_ratio = 0.35, 0.30, 0.35
        elif "–Ω–∞–±–æ—Ä" in goal.lower():
            protein_ratio, fat_ratio, carb_ratio = 0.30, 0.25, 0.45
        else:
            protein_ratio, fat_ratio, carb_ratio = 0.30, 0.30, 0.40
        
        daily_protein = int((daily_calories * protein_ratio) / 4)
        daily_fats = int((daily_calories * fat_ratio) / 9)
        daily_carbs = int((daily_calories * carb_ratio) / 4)
        
        # Basic meal plan
        meal_calories = daily_calories // 4
        
        return {
            "goal": goal,
            "daily_calories": daily_calories,
            "daily_protein": daily_protein,
            "daily_fats": daily_fats,
            "daily_carbs": daily_carbs,
            "days": [
                {
                    "day_name": day,
                    "meals": [
                        {
                            "name": "–ó–∞–≤—Ç—Ä–∞–∫",
                            "time": "08:00",
                            "calories": meal_calories,
                            "dishes": [{"name": "–ó–∞–≤—Ç—Ä–∞–∫ –ø–æ –ø–ª–∞–Ω—É", "calories": meal_calories}]
                        },
                        {
                            "name": "–û–±–µ–¥", 
                            "time": "13:00",
                            "calories": meal_calories,
                            "dishes": [{"name": "–û–±–µ–¥ –ø–æ –ø–ª–∞–Ω—É", "calories": meal_calories}]
                        },
                        {
                            "name": "–£–∂–∏–Ω",
                            "time": "18:00", 
                            "calories": meal_calories,
                            "dishes": [{"name": "–£–∂–∏–Ω –ø–æ –ø–ª–∞–Ω—É", "calories": meal_calories}]
                        },
                        {
                            "name": "–ü–µ—Ä–µ–∫—É—Å",
                            "time": "21:00",
                            "calories": meal_calories,
                            "dishes": [{"name": "–ü–µ—Ä–µ–∫—É—Å –ø–æ –ø–ª–∞–Ω—É", "calories": meal_calories}]
                        }
                    ]
                }
                for day in ["–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫", "–í—Ç–æ—Ä–Ω–∏–∫", "–°—Ä–µ–¥–∞", "–ß–µ—Ç–≤–µ—Ä–≥", "–ü—è—Ç–Ω–∏—Ü–∞", "–°—É–±–±–æ—Ç–∞", "–í–æ—Å–∫—Ä–µ—Å–µ–Ω—å–µ"]
            ]
        }

    async def generate_nutrition_plan(
        self,
        nutrition_data: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è –ø–ª–∞–Ω–∞ –ø–∏—Ç–∞–Ω–∏—è
        
        Args:
            nutrition_data: –î–∞–Ω–Ω—ã–µ –æ –ø–∏—Ç–∞–Ω–∏–∏ –∫–ª–∏–µ–Ω—Ç–∞
        
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π –ø–ª–∞–Ω –ø–∏—Ç–∞–Ω–∏—è –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ
        """
        
        # –í–∞–ª–∏–¥–∞—Ü–∏—è –≤—Ö–æ–¥–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
        required_fields = ["nutrition_goal", "daily_calories"]
        for field in required_fields:
            if field not in nutrition_data:
                raise ValidationError(f"–û—Ç—Å—É—Ç—Å—Ç–≤—É–µ—Ç –æ–±—è–∑–∞—Ç–µ–ª—å–Ω–æ–µ –ø–æ–ª–µ: {field}")
        
        try:
            # Try LLM generation first
            preferences_str = ", ".join(nutrition_data.get("food_preferences", ["–æ–±—ã—á–Ω–æ–µ –ø–∏—Ç–∞–Ω–∏–µ"]))
            allergies_str = ", ".join(nutrition_data.get("allergies", ["–Ω–µ—Ç"]))
            
            user_prompt = f"""
            –°–æ–∑–¥–∞–π –ø—Ä–æ—Å—Ç–æ–π –ø–ª–∞–Ω –ø–∏—Ç–∞–Ω–∏—è:
            –¶–µ–ª—å: {nutrition_data["nutrition_goal"]}
            –ö–∞–ª–æ—Ä–∏–∏: {nutrition_data["daily_calories"]}
            
            –§–æ—Ä–º–∞—Ç JSON:
            {{
                "goal": "{nutrition_data['nutrition_goal']}",
                "daily_calories": {nutrition_data["daily_calories"]},
                "daily_protein": 150,
                "daily_fats": 80,
                "daily_carbs": 300,
                "days": [
                    {{
                        "day_name": "–ü–æ–Ω–µ–¥–µ–ª—å–Ω–∏–∫",
                        "meals": [
                            {{
                                "name": "–ó–∞–≤—Ç—Ä–∞–∫",
                                "calories": 500,
                                "dishes": [{{
                                    "name": "–û–≤—Å—è–Ω–∫–∞",
                                    "calories": 300
                                }}]
                            }}
                        ]
                    }}
                ]
            }}
            """
            
            messages = [
                {
                    "role": "system",
                    "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–∏—Ç–∞–Ω–∏—é. –û—Ç–≤–µ—á–∞–π –¢–û–õ–¨–ö–û –≤–∞–ª–∏–¥–Ω—ã–º JSON."
                },
                {
                    "role": "user",
                    "content": user_prompt
                }
            ]
            
            result = await self._make_openai_request(
                messages=messages,
                request_type=LLMRequestType.PROGRAM_CREATE,
                max_tokens=1500  # Reduced to encourage simpler response
            )
            
            try:
                nutrition_plan = json.loads(result["content"])
                
                return {
                    "plan": nutrition_plan,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "llm"
                    }
                }
                
            except json.JSONDecodeError as e:
                logger.warning(f"LLM –æ—Ç–≤–µ—Ç –Ω–µ –≤–∞–ª–∏–¥–Ω—ã–π JSON: {e}")
                logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–ª–∞–Ω –ø–∏—Ç–∞–Ω–∏—è")
                
                # Use fallback plan
                fallback_plan = self._create_fallback_nutrition_plan(nutrition_data)
                
                return {
                    "plan": fallback_plan,
                    "metadata": {
                        "tokens_used": result["usage"]["total_tokens"],
                        "model": result["model"],
                        "latency_ms": result["latency_ms"],
                        "source": "fallback"
                    }
                }
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –ø–ª–∞–Ω–∞ –ø–∏—Ç–∞–Ω–∏—è: {e}")
            logger.info("–ò—Å–ø–æ–ª—å–∑—É–µ–º fallback –ø–ª–∞–Ω –ø–∏—Ç–∞–Ω–∏—è")
            
            # Create fallback plan
            fallback_plan = self._create_fallback_nutrition_plan(nutrition_data)
            
            return {
                "plan": fallback_plan,
                "metadata": {
                    "tokens_used": 0,
                    "model": "fallback",
                    "latency_ms": 0,
                    "source": "fallback"
                }
            }
    
    async def generate_shopping_list(
        self,
        nutrition_plan: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Å–ø–∏—Å–∫–∞ –ø–æ–∫—É–ø–æ–∫ –Ω–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ –ø–∏—Ç–∞–Ω–∏—è
        
        Args:
            nutrition_plan: –ü–ª–∞–Ω –ø–∏—Ç–∞–Ω–∏—è
        
        Returns:
            –°—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Å–ø–∏—Å–æ–∫ –ø–æ–∫—É–ø–æ–∫
        """
        
        user_prompt = f"""
        –ù–∞ –æ—Å–Ω–æ–≤–µ –ø–ª–∞–Ω–∞ –ø–∏—Ç–∞–Ω–∏—è —Å–æ–∑–¥–∞–π —Å–ø–∏—Å–æ–∫ –ø–æ–∫—É–ø–æ–∫ –Ω–∞ –Ω–µ–¥–µ–ª—é.
        
        –ü–ª–∞–Ω –ø–∏—Ç–∞–Ω–∏—è:
        {json.dumps(nutrition_plan, ensure_ascii=False, indent=2)}
        
        –°–æ–∑–¥–∞–π —Å–ø–∏—Å–æ–∫ –≤ JSON —Ñ–æ—Ä–º–∞—Ç–µ:
        {{
            "shopping_list": {{
                "–º–æ–ª–æ—á–Ω—ã–µ_–ø—Ä–æ–¥—É–∫—Ç—ã": [
                    {{"name": "–ú–æ–ª–æ–∫–æ", "quantity": "2–ª"}},
                    {{"name": "–¢–≤–æ—Ä–æ–≥", "quantity": "500–≥"}}
                ],
                "–º—è—Å–æ_—Ä—ã–±–∞": [
                    {{"name": "–ö—É—Ä–∏–Ω–∞—è –≥—Ä—É–¥–∫–∞", "quantity": "1–∫–≥"}}
                ],
                "–æ–≤–æ—â–∏_—Ñ—Ä—É–∫—Ç—ã": [
                    {{"name": "–ü–æ–º–∏–¥–æ—Ä—ã", "quantity": "500–≥"}}
                ],
                "–∫—Ä—É–ø—ã_–±–æ–±–æ–≤—ã–µ": [
                    {{"name": "–ì—Ä–µ—á–∫–∞", "quantity": "500–≥"}}
                ],
                "–ø—Ä–æ—á–µ–µ": [
                    {{"name": "–°–æ–ª—å", "quantity": "1 –ø–∞—á–∫–∞"}}
                ]
            }}
        }}
        
        –£—á–∏—Ç—ã–≤–∞–π:
        - –û–±—ä–µ–¥–∏–Ω–∏ –æ–¥–∏–Ω–∞–∫–æ–≤—ã–µ –ø—Ä–æ–¥—É–∫—Ç—ã
        - –†–∞—Å—Å—á–∏—Ç–∞–π –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –Ω–∞ –Ω–µ–¥–µ–ª—é
        - –ì—Ä—É–ø–ø–∏—Ä—É–π –ø–æ –∫–∞—Ç–µ–≥–æ—Ä–∏—è–º
        - –£–∫–∞–∂–∏ —É–¥–æ–±–Ω—É—é –¥–ª—è –ø–æ–∫—É–ø–∫–∏ —É–ø–∞–∫–æ–≤–∫—É
        """
        
        messages = [
            {
                "role": "system", 
                "content": "–¢—ã —ç–∫—Å–ø–µ—Ä—Ç –ø–æ –ø–ª–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—é –ø–æ–∫—É–ø–æ–∫ –ø—Ä–æ–¥—É–∫—Ç–æ–≤ –ø–∏—Ç–∞–Ω–∏—è."
            },
            {
                "role": "user",
                "content": user_prompt
            }
        ]
        
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.CHAT,
            max_tokens=1000
        )
        
        try:
            shopping_data = json.loads(result["content"])
            
            return {
                "shopping_list": shopping_data["shopping_list"],
                "metadata": {
                    "tokens_used": result["usage"]["total_tokens"],
                    "model": result["model"],
                    "latency_ms": result["latency_ms"]
                }
            }
            
        except json.JSONDecodeError as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ —Å–ø–∏—Å–∫–∞ –ø–æ–∫—É–ø–æ–∫: {e}")
            raise LLMServiceError("–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Å–ø–∏—Å–∫–∞ –ø–æ–∫—É–ø–æ–∫")

    async def get_completion(
        self,
        prompt: str,
        system_message: str = None,
        **kwargs
    ) -> str:
        """
        Simple method to get completion from LLM
        
        Args:
            prompt: User prompt
            system_message: Optional system message
            **kwargs: Additional parameters
        
        Returns:
            LLM response content
        """
        
        if not prompt.strip():
            raise ValidationError("–ü—Ä–æ–º–ø—Ç –Ω–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø—É—Å—Ç—ã–º")
        
        # –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–æ–æ–±—â–µ–Ω–∏–π
        messages = []
        
        if system_message:
            messages.append({
                "role": "system",
                "content": system_message
            })
        
        messages.append({
            "role": "user", 
            "content": prompt
        })
        
        # –í—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        result = await self._make_openai_request(
            messages=messages,
            request_type=LLMRequestType.CHAT,
            **kwargs
        )
        
        return result["content"].strip()


# –ì–ª–æ–±–∞–ª—å–Ω—ã–π —ç–∫–∑–µ–º–ø–ª—è—Ä —Å–µ—Ä–≤–∏—Å–∞
llm_service = LLMService() 